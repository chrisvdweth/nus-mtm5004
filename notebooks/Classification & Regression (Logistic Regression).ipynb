{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/div/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification & Regression: Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical modeling technique used for binary classification tasks, where the goal is to predict the probability of an instance belonging to a certain class. Despite its name, logistic regression is a classification algorithm, not a regression algorithm.\n",
    "\n",
    "In Logistic Regression, the algorithm models the relationship between the independent variables and the binary outcome using a logistic function, also known as the sigmoid function. The logistic function maps the linear combination of the independent variables to a value between 0 and 1, which represents the probability of belonging to the positive class. This mapping allows logistic regression to estimate the likelihood of an instance belonging to a class and make predictions accordingly.\n",
    "\n",
    "Although Logistic Regression is called \"regression,\" it is considered a linear model due to its underlying mathematical formulation. The linearity in Logistic Regression refers to the relationship between the independent variables and the log-odds (also known as logit) of the positive class. The log-odds are transformed using the logistic function, which introduces the nonlinearity necessary to model the probability.\n",
    "\n",
    "The linear part of Logistic Regression comes from the fact that the log-odds of the positive class are expressed as a linear combination of the independent variables. The algorithm determines the coefficients (weights) associated with each independent variable, similar to linear regression. However, instead of predicting the actual continuous value, logistic regression predicts the probability of belonging to the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Specify How Plots Get Rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Toy Data (CSI Example)\n",
    "\n",
    "As we did in the lecture, we adopt the simple CSI example we used for Linear Regression to a classification task. While the input is still the shoe print size of a person, the output is now a binary class label representing the sex of the person (woman: 0, man: 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    (31.3, 1), (29.7, 1), (31.3, 0), (31.8, 0),\n",
    "    (31.4, 1), (31.9, 1), (31.8, 1), (31.0, 1),\n",
    "    (29.7, 0), (31.4, 1), (32.4, 1), (33.6, 1),\n",
    "    (30.2, 0), (30.4, 0), (27.6, 0), (31.8, 1),\n",
    "    (31.3, 1), (34.5, 1), (28.9, 0), (28.2, 0)\n",
    "])\n",
    "\n",
    "# Convert input and outputs to numpy arrays; makes some calculations easier\n",
    "X = data[:,0].reshape(-1,1)\n",
    "y = data[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still plot the data by using the class label as y coordinate in the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('Shoe print size (cm)', fontsize=16)\n",
    "plt.ylabel('P(male)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is the expected trend that men generally have a larger show print size. But of course, there is no clear separation as there are tall women and small men with corresponding shoe print sizes. That means there will never be a perfect classifier to predict the sex of a person merely based on the size of a shoe print."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Logistic Regression\n",
    "\n",
    "scikit-learn provides [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) as implementation for Logistic Regression. Similar to the Linear Regression implementation, the model considers $\\theta_0$ (`intercept_`) and $\\theta_{i\\neq 0}$ (`coef_`) separately. It also features the parameter `fit_intercept` whether to calculate the intercept $\\theta_0$ or not.\n",
    "\n",
    "Below, as we use the original data without adding the constant term ourselves, should set `fit_intercept=True`, which is the default value, so we can simply ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X, y)\n",
    "\n",
    "print('Intercept: {}, Coefficients: {}'.format(clf.intercept_, clf.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this result in two ways:\n",
    "\n",
    "* directly plotting the probabilities (see orange line in the plot below)\n",
    "* plotting the decision boundary as defined by values for $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify series of shoe print size in the range of the input data\n",
    "x_range = np.arange(27, 35, 0.1).reshape(-1, 1)\n",
    "\n",
    "# Calculate the probability for all shoe print size\n",
    "# The method predict_proba() does this for us\n",
    "y_best = clf.predict_proba(x_range)[:,1]\n",
    "\n",
    "# Calculate the decision boundary\n",
    "decision_boundary = clf.intercept_ +  clf.coef_[0] * x_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the probability values and the decision boundary together with the data sample in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.scatter(X, y, c='C0', s=100)\n",
    "plt.plot(x_range, y_best, color='orange', lw=3)\n",
    "plt.plot(x_range, decision_boundary, '--', color='black', lw=2)\n",
    "plt.xlabel('Shoe print size (cm)', fontsize=16)\n",
    "plt.ylabel('P(male)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the classification is not perfect.\n",
    "\n",
    "### Predict Sex of Suspect\n",
    "\n",
    "In our CSI example, the shoe print size we found of the suspect was 32.2 cm. So take this value as input for our model and look at the prediction. The method `predict()` directly returns the predicted class label (instead of the probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict([[32.2]])\n",
    "\n",
    "print('The predicted class label is: {}'.format(y_pred.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class label on 1 means the suspect is predicted to be a man. This output can already be seen when looking at the plot above. Also recall that the predicted height for the suspect was 185.7 cm (see notebook for Linear Regression) which is arguably more likely to be a man.\n",
    "\n",
    "Apart from directly getting the class label, we can also look at the estimated probabilities. This gives us an indication of how \"sure\" the classifier is about the returned label. Again, we use the method `predict_proba()` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba([[32.2]])\n",
    "\n",
    "print('The estimated probabilites are: {}'.format(y_pred.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our suspect, the difference between the two probabilities is quite large, so we can be reasonably confident that the suspect is indeed a man -- although there will never be a 100% guarantee. Of course, both probabilities add up to 1.\n",
    "\n",
    "Let's assume the size of the shoe print size of the suspect would have been 30.6 cm. We can estimate the probabilities for this value as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba([[30.6]])\n",
    "\n",
    "print('The estimated probabilites are: {}'.format(y_pred.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the probability for Class 1 is still higher than for Class 0, we still would predict the suspect to be a man. However, here the two probabilities are much closer, so we can say the level of confidence of the classifier is much lower. This kind of interpretation is pretty straightforward for binary classification but gets less obvious for multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression using Vessel Details Dataset\n",
    "\n",
    "For a more practical example, let's see if we can predict the type of a vessel based on some of its features. This implies that the underlying assumption is that a vessel's width, length, and tonnage are good indicators for the vessel's type. This may not be obvious, but in the context of this notebook we will check how much this assumption will hold.\n",
    "\n",
    "### Prepared Training & Test Data\n",
    "\n",
    "#### Load Dataset from File\n",
    "\n",
    "As usual, we use `pandas` to load the `csv` file with the details about all vessels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/datasets/vessels/vessel-details.csv')\n",
    "\n",
    "# Sort dataset (often a good practice)\n",
    "df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "# Show the first 5 columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Selection\n",
    "\n",
    "To skip any more sophisticated data preprocessing steps, we consider only the convenient features -- that is, we consider only a subset of numerical features for our model. This particularly means that we do not have to consider any encoding strategies for categorical features. To keep it even simpler, we also remove all rows containing any missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the numerical attributes to keep it simple here + Type as our class label\n",
    "df = df[['Length', 'Width', 'Gross Tonnage', 'Deadweight Tonnage', 'Type']]\n",
    "\n",
    "# Remove all rows with any NaN values; again, just to keep it simple\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many class labels we have -- which is the number of unique labels of column `Type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#Classes: {}'.format(len(set(df.Type.tolist()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Class Labels\n",
    "\n",
    "Most classification algorithms assume that the class labels of the range 0..C, where C is the number of classes. Using `pandas`, this conversion is easy to do. After the conversion, all rows with the class labels, say, \"Oil Tanker\" will have the same numerical (integer) class label of the range 0..C. For our dataset, the number of classes is `C=15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type'] = pd.factorize(df['Type'])[0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Training & Test Data\n",
    "\n",
    "As usual, we convert the dataframe into numpy arrays for further processing, including splitting the dataset into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to numpy arrays\n",
    "X = df[['Length', 'Width', 'Gross Tonnage', 'Deadweight Tonnage']].to_numpy()\n",
    "y = df[['Type']].to_numpy().squeeze()\n",
    "\n",
    "# Split dataset in to training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))\n",
    "print(\"Size of test: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data via Standardization\n",
    "\n",
    "Since we want to consider different polynomial degrees, it is strongly recommended – and almost required – to normalize/standardize the data. As the [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) implementation also applies regularization by default, we do normalize the data via standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit the scaler based on the training data only\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Of course, we need to convert both training and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Logistic Regression Classifier\n",
    "\n",
    "We directly look into Polynomial Logistic Regression and try different maximum polynomial degrees $p$ (similar to the Linear Regression notebook). Recall from the lecture that the number of terms given a polynomial degree of $p$ a number of input features $d$ is\n",
    "\n",
    "$$\n",
    "\\#terms = \\binom{p+d}{p}\n",
    "$$\n",
    "\n",
    "Since our dataset has 8 input features, this equation simplifies to\n",
    "\n",
    "$$\n",
    "\\#terms = \\binom{p+5}{5}\n",
    "$$\n",
    "\n",
    "Below we consider $p$ as our hyperparameter, i.e., we transform the dataset using different polynomial degrees, apply Logistic Regression, and check the f1 score for each setup. Note that we no have to set `fit_intercept=False` as  [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) adds the constant term to the data matrix even if $p=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for p in range(1, 9):\n",
    "    \n",
    "    # Transform data w.r.t to degree of polynomial p\n",
    "    poly = PolynomialFeatures(p)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.fit_transform(X_test)\n",
    "    \n",
    "    # Train Linear Regressor or transformed data\n",
    "    # fit_intercept=False since for p=1, transformation adds constant term to data\n",
    "    poly_reg = LogisticRegression(fit_intercept=False, max_iter=1000).fit(X_train_poly, y_train)\n",
    "\n",
    "    # Predict values for training and test set\n",
    "    y_train_pred = poly_reg.predict(X_train_poly)\n",
    "    y_test_pred = poly_reg.predict(X_test_poly)\n",
    "    \n",
    "    # Calculate MSE \n",
    "    f1_train = f1_score(y_train, y_train_pred, average='micro')\n",
    "    f1_test = f1_score(y_test, y_test_pred, average='micro')\n",
    "    \n",
    "    \n",
    "    print('Degree of polynomial: {} => f1 (train/test): {:.2f}/{:.2f} (#terms: {})'.format(p, f1_train, f1_test, X_train_poly.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that setting `p=5` yields the highest average f1 score.\n",
    "\n",
    "Lastly, we can also perform a more proper evaluation using k-fold cross-validation to find the best value of $p$. To simplify things, we use the scikit-learn's method `cross_val_score()` to perform the cross-validation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize the best f1-score and respective k value\n",
    "best_p, best_f1 = None, 0.0\n",
    "\n",
    "# Loop over a range of values for setting k\n",
    "for p in range(1, 9):\n",
    "    \n",
    "    # Transform data w.r.t to degree of polynomial p\n",
    "    poly = PolynomialFeatures(p)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.fit_transform(X_test)\n",
    "    \n",
    "    # Specfify type of classifier\n",
    "    clf = LogisticRegression(fit_intercept=False, max_iter=1000)\n",
    "    \n",
    "    # perform cross validation (here with 5 folds)\n",
    "    # f1_scores is an array containg the 5 f1-scores\n",
    "    f1_scores = cross_val_score(clf, X_train_poly, y_train, cv=5)\n",
    "    \n",
    "    # Calculate the f1-score for the current k value as the mean over all 5 f1-scores\n",
    "    f1_fold_mean = np.mean(f1_scores)\n",
    "\n",
    "    print('p={}, f1 score (mean/std): {:.3f}/{:.3f}'.format(p, f1_fold_mean, np.std(f1_scores)))\n",
    "    \n",
    "    # Keep track of the best f1-score and the respective k value\n",
    "    if f1_fold_mean > best_f1:\n",
    "        best_p, best_f1 = p, f1_fold_mean\n",
    "  \n",
    "\n",
    "print('The best f1-score was {:.3f} for p={}'.format(best_f1, best_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, $p=5$ yields the best average f1 score. Having found the best value for $p$ we can now fit a Logistic Regression model on the whole training data and $p=5$ and evaluate the f1 score on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "poly = PolynomialFeatures(5)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "# Fite Logistic Regression model on complete training data\n",
    "clf = LogisticRegression(fit_intercept=False, max_iter=1000).fit(X_train_poly, y_train)\n",
    "\n",
    "# Predict class labels for test data\n",
    "y_pred = clf.predict(X_test_poly)\n",
    "\n",
    "# Calculate f1 scores based on ground truth of test set\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print('F1 score of Linear Regression model on the test data: {:.3f}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Logistic Regression is a popular machine learning algorithm used for binary classification tasks, where the goal is to predict the probability of an instance belonging to a certain class. Despite its name, logistic regression is a classification algorithm, not a regression algorithm. It is called \"Logistic Regression\" because it is based on the concept of logistic function or sigmoid function.\n",
    "\n",
    "In Logistic Regression, the algorithm models the relationship between the independent variables and the binary outcome using a logistic function. The logistic function maps the linear combination of the independent variables to a value between 0 and 1, which represents the probability of belonging to the positive class. This mapping allows logistic regression to estimate the likelihood of an instance belonging to a class and make predictions accordingly.\n",
    "\n",
    "One of the key advantages of Logistic Regression is its simplicity and interpretability. The algorithm provides coefficients for each independent variable, allowing us to understand the impact and direction of each variable on the probability of the positive class. However, logistic regression has some limitations. It assumes a linear relationship between the independent variables and the log-odds of the positive class, which may not always hold true. It may struggle with nonlinear relationships or complex interactions between variables. Additionally, logistic regression is sensitive to outliers and can be affected by overfitting when the number of independent variables is large compared to the number of instances.\n",
    "\n",
    "In summary, logistic regression is a straightforward and interpretable algorithm for binary classification tasks. Its pros include simplicity, interpretability, and handling of categorical and continuous variables. However, it has limitations in capturing nonlinear relationships and can be sensitive to outliers and overfitting. Therefore, it is important to assess the assumptions and limitations of logistic regression before applying it to a given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/div/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering: AGNES\n",
    "\n",
    "The AGNES (Agglomerative Nesting) clustering algorithm is an agglomerative hierarchical clustering algorithm used to group similar objects or data points into clusters. The algorithm follows a bottom-up approach, starting with each data point as a separate cluster and then iteratively merging the closest clusters until a termination condition is met. Here's a step-by-step overview of the AGNES clustering algorithm:\n",
    "\n",
    "* Begin by considering each data point as a separate cluster.\n",
    "* Compute the proximity matrix, which represents the distance or dissimilarity between each pair of clusters.\n",
    "* Find the two closest clusters based on the proximity matrix. This can be done using various distance metrics such as Euclidean distance or Manhattan distance.\n",
    "* Merge the two closest clusters into a single cluster.\n",
    "* Update the proximity matrix by recalculating the distances between the merged cluster and the remaining clusters. This step is crucial as it affects the subsequent cluster merging decisions.\n",
    "* Repeat steps 3-5 until a termination condition is met. This condition could be a predefined number of clusters or a distance threshold that determines the maximum dissimilarity allowed for cluster merging.\n",
    "* The algorithm terminates when all data points belong to a single cluster or the termination condition is satisfied.\n",
    "\n",
    "AGNES produces a hierarchical clustering structure in the form of a dendrogram, which represents the merging order of clusters. The dendrogram can be cut at a desired level to obtain a specific number of clusters.\n",
    "\n",
    "One limitation of AGNES is its computational complexity, especially for large datasets, as the algorithm requires calculating the proximity matrix at each iteration. Another consideration is the choice of distance metric, as it can significantly impact the clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Specify how Plots Get Rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with Toy Data\n",
    "\n",
    "We use the very small and manually crafted dataset shown in the lecture slides to keep it simple. Otherwise, the corresponding distance matrix and dendrogram get quickly uncomfortably large to easily comprehend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datset with the 6 data points\n",
    "X_demo = np.array([ (-6,1), (-4.8,2.9), (-2,-2.5), (3,2), (2.7,-3.5), (3,-2) ])\n",
    "\n",
    "# Label the date points from 0..5\n",
    "labels_demo = list(range(X_demo.shape[0]))\n",
    "\n",
    "# Plot all data points with their labels\n",
    "plt.figure()\n",
    "plt.scatter(X_demo[:,0], X_demo[:,1])\n",
    "for i, label in enumerate(labels_demo):\n",
    "    plt.gca().annotate(label, X_demo[i], fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run [`sklearn.cluster.AgglomerativeClustering`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) of scikit-learn. We set the `distance_threshold=0` and `n_clusters=None` to calculate and return the full hierarchy of clusters. We also use *Single Linkage* for the linkage method. You can check out the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) to see the purpose and effects of different input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnes_demo = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage='single').fit(X_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the clustering is no longer partitional, plotting the results becomes a bit more challenging. But again, scikit-learn provides useful tools to plot dendrograms. A dendrogram is a hierarchical tree-like diagram commonly used to visualize the results of hierarchical clustering algorithms, such as AGNES. It illustrates the arrangement of clusters and their relationships at different levels of similarity or dissimilarity. Dendrograms are particularly useful for understanding the hierarchical structure of the data and identifying the optimal number of clusters.\n",
    "\n",
    "In a dendrogram, the data points or clusters are represented as leaves or individual branches at the bottom of the diagram. The merging of clusters is depicted through the formation of branches that join together at higher levels of the dendrogram. The height or length of each branch in the dendrogram corresponds to the dissimilarity or distance between the merged clusters. Here's a brief explanation of how to interpret a dendrogram:\n",
    "\n",
    "* The vertical axis represents the dissimilarity or distance measure used in the clustering algorithm. The scale varies depending on the specific distance metric employed.\n",
    "* Each branch in the dendrogram represents the merging of clusters at a particular level. The height of the branch indicates the dissimilarity between the clusters being merged. Higher branches indicate larger dissimilarities.\n",
    "* The order of merging clusters is shown by tracing the branches backward from the leaves towards the root of the dendrogram.\n",
    "* The horizontal lines in the dendrogram represent the cutoff points that determine the number of clusters. By selecting a horizontal line and cutting the dendrogram at that level, clusters can be obtained.\n",
    "\n",
    "By visually examining the dendrogram, one can identify different levels of similarity and determine the appropriate number of clusters based on the desired level of granularity. The dendrogram provides an intuitive representation of the clustering process and enables insights into the hierarchical relationships among clusters. \n",
    "\n",
    "The following function is directly adopted from [`scikit-learn` website](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    plt.figure()\n",
    "    \n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the method with `agnes_demo` to visualize the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(agnes_demo, truncate_mode='level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dendrogram should be intuitive to read. From the scatter plot above, we could see that Points 4 and 5 -- strictly speaking: the two clusters that contain Point 4 and Point 5 respectively -- were closest, and thus get merged first. This is followed by merging the two clusters containing Point 0 and Point 1 respectively. Then, according to the dendrogram, Cluster 4/5 got merged with Cluster 3. This might no longer be so obvious from looking at the scatter plot. But keep in mind that Single Linkage was used, so the distance between Point 3 and Point 5 mattered here.\n",
    "\n",
    "**Side note:** You can try different linkage methods and see how it affects the results. From the lecture, we already know that not all linkage methods will yield the same clustering.\n",
    "\n",
    "As mentioned above, a typical usage of `AgglomerativeClustering` is to specify the number of clusters (similar to K-Means). Try different values for `n_clusters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnes_demo = AgglomerativeClustering(n_clusters=3).fit(X_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and in this case we can plot the clustering as if it is partitional as we are only looking at the level of the hierarchy that resulted in `n_clusters` clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(agnes, data, point_size=50, show_ticks=True, aspect=None):\n",
    "    plt.figure()\n",
    "    \n",
    "    if aspect is not None:\n",
    "        plt.axes().set_aspect(aspect)\n",
    "    \n",
    "    # Plot all the data points a color-code them w.r.t. to their cluster label/id\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=agnes.labels_, s=point_size, cmap=plt.cm.tab10)\n",
    "    \n",
    "    if show_ticks is False:\n",
    "        plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)      \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(agnes_demo, X_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with Toy Data\n",
    "\n",
    "[`sklearn.datasets`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) provides a series of methods to randomly generate sample data. \n",
    "\n",
    "Try different methods and see how the results will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=5, n_features=2, cluster_std=0.85, random_state=11)\n",
    "#X, y = make_moons(n_samples=250, noise=0.105, random_state=0)\n",
    "#X, y = make_circles(n_samples=500, noise=0.06, factor=0.5, random_state=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Dendrogram\n",
    "\n",
    "In the code cell below, we first compute again the complete hierarchy and plot the dendrogram. Even with only 100 data points, the resulting dendrogram showing the full hierarchy already becomes quite cluttered and unreadable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Hierarchical Clustering (AGNES)\n",
    "agnes_toy = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(X)\n",
    "\n",
    "plot_dendrogram(agnes_toy, truncate_mode='level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Evaluation\n",
    "\n",
    "If we specify the number of clusters using the input parameter `n_clusters`, we can evaluate the resulting clusterings the same way as we have seen with K-Means and DBSCAN, namely calculating the Silhouette Score and Rand Index.\n",
    "\n",
    "Feel free to try out different linkage methods; see the documentation linked above. You also use different types of toy data and see their effects on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = []\n",
    "\n",
    "for n_clusters in tqdm(range(2, 20+1)):\n",
    "\n",
    "    # Run K-Means for the current number of clusters k\n",
    "    agnes = AgglomerativeClustering(n_clusters=n_clusters, linkage='single').fit(X)\n",
    "    \n",
    "    # silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed clusters    \n",
    "    silhouette.append((n_clusters, silhouette_score(X, agnes.labels_)))\n",
    "    \n",
    "# Convert to numpy array for convenience\n",
    "silhouette = np.array(silhouette)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's print the Silhouette Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of Clusters', fontsize=16)\n",
    "plt.ylabel('Silhouette Score', fontsize=18)\n",
    "plt.tick_params(axis=\"x\", labelsize=12)\n",
    "plt.tick_params(axis=\"y\", labelsize=12)\n",
    "plt.plot([s[0] for s in silhouette], [s[1] for s in silhouette], marker='o', lw=2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For `make_blobs()` with the default values:** Unsurprisingly, for `n_clusters=5`, we see the highest Silhouette Score. It is not surprising since our toy data consists of 5 rather well-separated clusters.\n",
    "\n",
    "To plot our best clustering, we can run Agnes with `n_clusters=5` and run the method `plot_clusters()`; see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnes_toy_best = AgglomerativeClustering(n_clusters=5).fit(X)\n",
    "\n",
    "plot_clusters(agnes_toy_best, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Data\n",
    "\n",
    "As a last example, we use the location data we have already seen in the DBSCAN notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/datasets/singapore/sg-places.csv', header=None)\n",
    "\n",
    "print(set(df[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick all the places of one type. The descriptions in the following will refer to restaurants (in more detail: McDonald's restaurants; see below). But feel free to change the type to anything of your interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different place types\n",
    "place_type = 'restaurant'\n",
    "#place_type = 'bus_station'\n",
    "#place_type = 'subway_station' # MRT+LRT stations\n",
    "#place_type = 'store'\n",
    "\n",
    "df_places = df[df[2]==place_type]\n",
    "\n",
    "print(df_places.shape)\n",
    "\n",
    "df_places.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3k+ restaurants is quite a lot, and maybe even too diverse for an analysis. So let's further filter the dataset to only include McDonald's restaurants. We use regular expressions for that. No worries, if you're not familiar with regular expressions. In a nutshell, they allow for filtering using flexible substring matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(r'mcdonald', flags=re.IGNORECASE)\n",
    "df_places = df_places[[bool(p.search(x)) for x in df_places[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_restaurants, num_attributes = df_places.shape\n",
    "\n",
    "print('Number of places: {}'.format(num_restaurants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the column containing latitude and longitude to a matrix, i.e., a 2d numpy array for further processing. Note that the resulting order is \\[longitude, latitude\\], since longitude represents the x variable and latitude the y variable. This doesn't matter for the clustering but it ensures that the plots look alright and are not rotated by 90 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_places = df_places[[4, 3]].to_numpy()\n",
    "\n",
    "print(X_places[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below provides a neat way that the proportions of the plotted points look nicer. Otherwise, the induced shape of Singapore will be squashed. Since Singapore is so close to the equator, this correction is not really needed, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect = 1/np.cos(np.radians(1.0))\n",
    "print(aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot all the places (e.g., the 141 McDonald's restaurant). You should be able to recognize the outline of Singapore. Of course, if you pick place types that are much less common and/or can only be found in certain areas, you won't be able to \"see\" the outline of Singapore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.axes().set_aspect(aspect)\n",
    "plt.scatter(X_places[:,0], X_places[:,1], s=25)\n",
    "plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can have a look and the complete dendrogram show the complete hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnes_places = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(X_places)\n",
    "\n",
    "plot_dendrogram(agnes_places, truncate_mode='level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but often, we specify the number of clusters; the code cell below uses 10 clusters, but feel free to change this to see how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnes_places = AgglomerativeClustering(n_clusters=10).fit(X_places)\n",
    "\n",
    "plot_clusters(agnes_places, X_places, show_ticks=False, aspect=aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this data is not labeled, we can basically only check the silhouette scores for different numbers of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette= []\n",
    "for n_clusters in tqdm(range(2, 40+1)):\n",
    "\n",
    "    # Run K-Means for the current number of clusters k\n",
    "    agnes = AgglomerativeClustering(n_clusters=n_clusters).fit(X_places)\n",
    "    \n",
    "    # silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed clusters    \n",
    "    silhouette.append((n_clusters, silhouette_score(X_places, agnes.labels_)))\n",
    "    \n",
    "# Convert to numpy array for convenience\n",
    "silhouette = np.array(silhouette)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we can plot the Silhouette scores for different numbers of clusters to inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of Clusters', fontsize=16)\n",
    "plt.ylabel('Silhouette Score', fontsize=18)\n",
    "plt.tick_params(axis=\"x\", labelsize=12)\n",
    "plt.tick_params(axis=\"y\", labelsize=12)\n",
    "plt.plot([s[0] for s in silhouette], [s[1] for s in silhouette], marker='o', lw=2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset (assuming the 141 McDonald's restaurants), the results are less straightforward -- that is, there is no (very) clear best choice for the number of clusters. Again, this is not that surprising seeing that the data points are much more scattered comparedto our toy data above. However, strictly speaking there is one maximum Silhouette Score, and we can check which choice of `n_clusters` resulted in that score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = np.argmax(silhouette[:,1])\n",
    "best_n_clusters = int(silhouette[best_run][0])\n",
    "\n",
    "print('Number of clusters with the highest Silhouette Score: {}'.format(best_n_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to, we can again compute AGNES using this best parameter value and plot all clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnes_places_best = AgglomerativeClustering(n_clusters=best_n_clusters).fit(X_places)\n",
    "\n",
    "plot_clusters(agnes_places_best, X_places, show_ticks=False, aspect=aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "AGNES (Agglomerative Nesting) clustering algorithm is an agglomerative hierarchical clustering approach that starts with each data point as a separate cluster and iteratively merges the closest clusters until a termination condition is met. One of the advantages of AGNES is its ability to handle non-globular and irregularly shaped clusters, making it suitable for a wide range of data distributions. It produces a dendrogram, which provides a hierarchical structure of the clusters and allows flexibility in selecting the desired number of clusters by cutting the dendrogram at an appropriate level.\n",
    "\n",
    "However, AGNES has a few limitations. First, its computational complexity can be high, especially for large datasets, as it requires calculating the proximity matrix at each iteration. This can result in increased time and memory requirements. Second, AGNES is sensitive to the choice of distance metric. The selection of an appropriate distance measure is crucial as it can significantly affect the clustering results. Additionally, AGNES suffers from the \"chaining\" effect, where once two clusters are merged, they cannot be separated again. This can lead to suboptimal cluster assignments, especially if the initial merging decisions were based on noisy or inaccurate data.\n",
    "\n",
    "In summary, AGNES is a hierarchical clustering algorithm that offers flexibility in determining the number of clusters through the use of a dendrogram. It can handle complex data distributions and provides insights into the hierarchical relationships among clusters. However, it has computational complexity concerns, is sensitive to the choice of distance metric, and may suffer from the chaining effect. These considerations should be taken into account when applying AGNES in practice, especially for large datasets or when dealing with noisy or ambiguous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511dd740-7605-4eb3-82cc-8f2712cd6d51",
   "metadata": {},
   "source": [
    "<img src=\"data/images/div/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47216e08-386a-4cf6-ac11-963257b01708",
   "metadata": {},
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "Image preprocessing is a fundamental stage in preparing image data for images analytics machine learning tasks, especially in computer vision. It involves a suite of operations applied to images before feeding them into models. This preparatory phase significantly impacts the performance and effectiveness of these models.\n",
    "\n",
    "The significance of image preprocessing lies in its ability to standardize, clean, and optimize raw image data. Normalization, for instance, ensures that pixel values across images are scaled uniformly, aiding in faster convergence during model training and preventing biases toward certain intensity ranges. Additionally, operations like noise reduction and cleaning eliminate irrelevant elements or artifacts, refining the data to help models extract relevant features more accurately. Resizing and rescaling ensure uniformity in image dimensions, simplifying computational complexity while maintaining consistency in input formats for models to learn effectively.\n",
    "\n",
    "Overall, image preprocessing acts as a crucial enabler, enhancing the quality of input data for machine learning models. By preparing images systematically, it empowers models to learn more efficiently, generalize better to new data, and produce more reliable and precise outputs in various computer vision tasks.\n",
    "\n",
    "Let's get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a193330-303a-4d6a-89cd-e5390eeffa17",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Importing Required Packages\n",
    "\n",
    "`torchvision` is a popular library in the PyTorch ecosystem primarily used for computer vision tasks. It provides tools and utilities for image and video processing, including datasets, image transformations, pre-trained models, and common image-based operations. Here are some advantages of `torchvision`:\n",
    "\n",
    "* **Datasets and Data Loaders:** `torchvision` offers easy access to standard datasets used in computer vision, such as MNIST, CIFAR-10, ImageNet, etc. It provides convenient data loaders to efficiently load and preprocess these datasets for training and testing neural networks.\n",
    "\n",
    "* **Image Transformations:** It offers a wide range of image transformations (such as cropping, resizing, normalization, etc.) that can be applied to datasets during training or inference. These transformations help in augmenting data, improving model generalization, and preprocessing images for neural network input.\n",
    "\n",
    "* **Pre-trained Models:** `torchvision` includes pre-trained state-of-the-art models like ResNet, VGG, AlexNet, etc., trained on large datasets like ImageNet. These models can be easily loaded and fine-tuned for specific tasks, saving time and computational resources.\n",
    "\n",
    "* **Utilities for Computer Vision:** It provides various utility functions for common computer vision tasks, such as image filtering, object detection, segmentation, and more. These utilities simplify the implementation of complex vision algorithms.\n",
    "\n",
    "* **Integration with PyTorch:** Being a part of the PyTorch ecosystem, `torchvision` seamlessly integrates with other PyTorch functionalities, allowing for easy incorporation of computer vision components into deep learning workflows.\n",
    "\n",
    "Overall, `torchvision` streamlines the development process for computer vision tasks by offering a set of pre-built tools, datasets, models, and transformations, which significantly simplifies the implementation and experimentation with deep learning models in PyTorch.\n",
    "\n",
    "In this notebook, we focus on **Image Transformations** as they include the important preprocessing steps of required to perform to prepare images for futher analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534faba4-a0f8-4d26-a219-4a2a7b2d1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795c832-d79b-4483-ab38-4ab0fe19281c",
   "metadata": {},
   "source": [
    "`torchvision.transforms` is a module within the `torchvision` library that offers a wide range of image transformations commonly used in computer vision tasks. These transformations can be applied to images or datasets to augment data, preprocess images, and prepare them for consumption by neural networks. The current version is `v2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865fbd3f-9c17-4f50-afac-cf527df58e1d",
   "metadata": {},
   "source": [
    "The Python Imaging Library, commonly known as `PIL`, is a library for performing basic image processing tasks in Python. However, it's important to note that the original `PIL` library hasn't been updated since 2011. Instead, its fork, known as the Python Imaging Library (PIL) fork, `Pillow`, has become the more commonly used and actively maintained library for image processing in Python.\n",
    "\n",
    "`Pillow` extends the capabilities of the original `PIL` library and provides a wide range of functionalities, including:\n",
    "\n",
    "* **Image Opening and Saving:** `Pillow` allows you to open and save various image file formats, such as JPEG, PNG, BMP, TIFF, and more.\n",
    "\n",
    "* **Image Manipulation:** It enables you to perform basic image manipulations like resizing, cropping, rotating, flipping, and transforming images.\n",
    "\n",
    "* **Image Filtering:** `Pillow` provides a set of filters and enhancements like blurring, sharpening, edge detection, and applying various effects to images.\n",
    "\n",
    "* **Color Space Manipulation:** You can convert images between different color spaces, such as RGB, grayscale, CMYK, etc.\n",
    "\n",
    "* **Image Drawing:** `Pillow` allows you to draw on images, add text, shapes, and annotations.\n",
    "\n",
    "* **Image Metadata Handling:** It supports handling image metadata, including EXIF data, allowing you to access and modify metadata information associated with images.\n",
    "\n",
    "Pillow (the Python Imaging Library fork) is widely used in the Python ecosystem for tasks related to image processing, computer vision, web development, scientific computing, and more due to its ease of use and extensive capabilities in handling and manipulating images. Here, we only need it to open and load images such as JPGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c2e27-c465-4ee1-8b90-70e03c720ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Jupyter notebook method display to render PIL images\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7dd131-75c4-4b1c-92bd-07f962ebeb0a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe5b54-ec60-48c0-b66a-3fae2e2befa0",
   "metadata": {},
   "source": [
    "## Load & Inspect Image\n",
    "\n",
    "### Load PIL Image from File\n",
    "\n",
    "The most important class in the Python Imaging Library is the `Image` class, defined in the module with the same name. You can create instances of this class in several ways; either by loading images from files, processing other images, or creating images from scratch. To load an image from a file, use the `open()` function in the `Image` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba560ee-4b4c-47e2-ab82-6d0df08c6def",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('data/images/examples/cruise-ship-01.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59021058-b32a-4bc0-aa34-1975522dbd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a255f-725a-49b2-be67-50c0de12a01f",
   "metadata": {},
   "source": [
    "### Check Basic Information\n",
    "\n",
    "We can now use instance attributes to examine the file contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e64cfa-436f-4622-9fcd-cd38580d1804",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Image format: {}'.format(image.format))\n",
    "print('Image size: {}'.format(image.size))\n",
    "print('Image mode: {}'.format(image.mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cdafe0-7bb2-4899-b40f-5262f8b9d21f",
   "metadata": {},
   "source": [
    "### Convert Image to Tensor\n",
    "\n",
    "After loading the image, it is stored as an internal `PIL` data structure. Most analytics algorithms, however, assume the images represented as a **tensor**, i.e., as a multidimensional array. The `torchvision.transforms` package provides the required function to convert a `PIL` image into its corresponding tensor representation.\n",
    "\n",
    "In the code cell below, we use `vs.Compose()` to define a list of preprocessing steps we want to perform on an image, and we wrap all steps as a new function we call `convert_image`. In later steps, we will extend this approach to include additional preprocessing steps that are commonly performed over images. Right now, we limit ourselves to the conversion to tensors (and ignore any resizing, cropping, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a277f1c-c0b1-428c-abc9-41c146f9711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_image = v2.Compose([\n",
    "    v2.ToImage(),                         # Convert to tensor, only needed if input is a PIL image\n",
    "    v2.ToDtype(torch.uint8, scale=True),  # Optional; most input are already uint8 at this \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cb4c6-849e-4e69-a3b5-b063bc149d29",
   "metadata": {},
   "source": [
    "Let's now apply the method `convert_image()` on our original input image and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac9778-b073-42a5-8226-3623198733fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_converted = convert_image(image)\n",
    "\n",
    "print(image_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a58c9-d89c-43fc-9477-652b0de581e9",
   "metadata": {},
   "source": [
    "The output of the code cell below shows an abbreviated version of the tensor. Recall that our image has 800x533 pixels and 3 color channels (Red, Green, Blue). This means that our tensor contains 800x500x3 = 1,200,000 entries, 3 entries for each pixel (again, given the 3 color channels). We can also explicitly get this information by looking at the `shape` of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9c325-8f0d-4d5d-ae23-7801ecbcb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_converted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0889f-1fa4-4e16-a901-0ff799eb56ce",
   "metadata": {},
   "source": [
    "This output shows that the first dimension reflects the number of color channels (3), the second dimension reflects the height of the image (533 pixels), and the third dimension reflects the width of the image (800 pixels). Since `image_converted` is now just a 3d tensor with numerical value, trying to use `display()` to show the image no longer works. Instead, it just prints the tensor again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a67f1-bb9e-447b-9fb1-4da00ad7d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6facae-b925-4870-bf41-61a40a722e8e",
   "metadata": {},
   "source": [
    "To actually show the image again, the `torchvision.transforms` package comes with an auxiliary function to convert the tensor back into a `PIL` image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661f31b-e91d-425b-b8ff-e8fdcf6b77b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_to_PIL = v2.ToPILImage()\n",
    "\n",
    "display(transform_to_PIL(image_converted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dacb03f-3c35-4f4a-9712-96c2fdf574e2",
   "metadata": {},
   "source": [
    "We will use the method `transform_to_PIL()` in the following to also visualize the image after performing various preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde96523-c90b-4c5f-bbc6-0a99292f4ae3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0610acf-5fff-476f-b37d-fc544edb19c0",
   "metadata": {},
   "source": [
    "## Basic Preprocessing\n",
    "\n",
    "Most data analytics require that all data samples have a fixed and predefined size. In the context of images analytics this means that all images must have the same size in terms of the number of pixels and their heights and widths. In practice, it is very common that images in a dataset have different sizes. We therefore need to convert all images to the same size. The most commonly applied to steps are:\n",
    "\n",
    "* **Resize:** Due to performance reasons, image analytics and machine learning tasks over image datasets are typically performed over smaller images. The first step is therefore to resize the image, again, typically to a smaller version compared to the original. Note that often the images are not resized to the final target size. The assumption is that the important parts of the image are typically more in the center than on the edges of the image.\n",
    "\n",
    "* **Crop:** By default, resizing does not change the aspect ratio of the image. However, in the end, all images need to have the same size in terms of height and width. The straightforward way to accomplish this is to crop the image. In a nutshell, cropping the image refers to removing certain outer parts of an image.\n",
    "\n",
    "Again, the `torchvision.transforms` package provides the required function to make performing these two steps very easy, as shown in the code cell below. We first use `v2.Resize` to resize the input image so that the shortest side (for our input image: the height) will have 256 pixels; the width of the image will be resized accordingly to preserve the aspect ratio. Then we apply `v2.CenterCrop` to extract a squared patch of the image of 224x224 pixels with respect to the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a29ef-80e2-4b90-a558-2c59ebdd9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_image = v2.Compose([\n",
    "    v2.ToImage(),                         # Convert to tensor, only needed if input is a PIL image\n",
    "    v2.ToDtype(torch.uint8, scale=True),  # Optional; most inputs are already in uint8\n",
    "    v2.Resize(256, antialias=True),       # Resize image so the shortest side has 256 pixels\n",
    "    v2.CenterCrop(224)                    # Crop out squared patch of size 224 pixels from the center\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e8680-f180-4f62-86f4-57a3b6934ade",
   "metadata": {},
   "source": [
    "**Side note:** In the example above, we choose a target size of 224x224 pixels as this is a common image size for many popular public image dataset, and many state-of-the-art machine learning models for image analytics have been trained using images of that size. However, keep in mind that there is nothing intrinsically special about this image size of 224x224!\n",
    "\n",
    "Let's apply the method `preprocess_image()` on our input image and have a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6460c-fa87-4ea1-889a-a78ffa4a51d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_preprocessed = preprocess_image(image)\n",
    "\n",
    "display(transform_to_PIL(image_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ade13-f45d-4be9-be30-3d6c7b594f0e",
   "metadata": {},
   "source": [
    "In a practical application, we can now apply the method `preprocess_image()` to all of our images that are part of our analysis, to ensure that the processed results of all images have all the same sizes, i.e., the same dimensions and the same number of pixels. This is a very common requirement for many image analytics such (e.g., image classification).\n",
    "\n",
    "**Side note:** You can easily see that particularly the cropping of the image might remove important parts. As mentioned before, the common assumption is that the most important parts of an image are more likely to be in the center than towards the edges. Of course, this assumption might always hold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af298f3c-9d8b-4197-87a0-2f8b57530ee1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0355f7-7924-45c4-a94c-3eb52340488c",
   "metadata": {},
   "source": [
    "### Advanced Preprocessing & Augmentation\n",
    "\n",
    "When applying the method `preprocess_image()` to an image, the output will always be the same. However, there are often reasons to randomize the performed preprocessing steps. The most common purpose to do this is called **Data Augmentation**. Data augmentation for images refers to a set of techniques used to create new training examples from existing ones by applying various transformations or modifications to the original images. It's a crucial step in training machine learning models, particularly in computer vision, to improve their generalization, robustness, and ability to handle different variations in the input data.\n",
    "\n",
    "Common examples of data augmentation techniques for images include:\n",
    "\n",
    "* **Rotation:** Rotating images by a certain angle (e.g., 90 degrees, 180 degrees) to simulate different orientations.\n",
    "\n",
    "* **Flip (Horizontal/Vertical):** Flipping images horizontally or vertically to create mirror images. For instance, flipping an image of a cat horizontally would show the cat facing the opposite direction.\n",
    "\n",
    "* **Random Crop:** Extracting random sections of the image to create variations in framing or object placement. This helps models become more tolerant to object positions within an image.\n",
    "\n",
    "* **Scaling and Resizing:** Changing the size of images while maintaining their aspect ratios. Scaling images up or down can help models generalize better to different object sizes.\n",
    "\n",
    "* **Translation:** Shifting an image along its horizontal or vertical axis. This can simulate changes in object location within an image.\n",
    "\n",
    "* **Brightness and Contrast Adjustment:** Modifying brightness, contrast, saturation, or hue of images to simulate different lighting conditions.\n",
    "\n",
    "* **Noise Injection:** Adding random noise to images to make models more robust to noise in real-world scenarios.\n",
    "\n",
    "* **Color Jitter:** Randomly altering color channels to change the appearance of images.\n",
    "\n",
    "* **Shearing:** Distorting images by shifting pixels in a certain direction, creating a sheared effect.\n",
    "\n",
    "* **Elastic Transformations:** Applying local deformations to images to simulate distortions or warping.\n",
    "\n",
    "These augmentation techniques help in increasing the diversity of the training dataset without collecting new data. By presenting modified versions of images during training, models become more robust and less sensitive to variations that might exist in the real-world data. However, it's essential to apply these transformations judiciously, considering the specific requirements of the task and the characteristics of the dataset, to avoid introducing unrealistic variations that could potentially confuse the model.\n",
    "\n",
    "The `torchvision.transforms` package provides a wide range of methods for data augmentation. Let's look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75dbeb9-122d-4e06-86f8-ad2da106afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_image = v2.Compose([\n",
    "    v2.ToImage(),                         # Convert to tensor, only needed if input is a PIL image\n",
    "    v2.ToDtype(torch.uint8, scale=True),  # Optional; most inputs are already in uint8 \n",
    "    v2.Resize(256, antialias=True),       # Resize image so the shortest sides has 256 pixels\n",
    "    v2.RandomCrop(224),                   # Crop out a squared patch of size 224 pixels from a random position\n",
    "    v2.RandomGrayscale(0.50),             # Convert to grayscale image with 50% probability\n",
    "    v2.ColorJitter(),                     # Adjust the contrast, saturation, hue, brightness, and also randomly permutes channels\n",
    "    v2.RandomHorizontalFlip(),            # Randomly flip image horizontally\n",
    "    v2.RandomErasing()                    # Randomly remove patch of the image (rectangular shape)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52367e56-4ba2-402c-82ef-57702d9ac85a",
   "metadata": {},
   "source": [
    "The code cell below appies the method `augment_image()` to our original input image. Run the code cell below multiple times to observe how the output of the method will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd975c68-58e0-406a-a29d-0a5a55f0dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_preprocessed = augment_image(image)\n",
    "\n",
    "display(transform_to_PIL(image_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f9d85-c181-4f98-a37e-bfe4419dc1ad",
   "metadata": {},
   "source": [
    "A complete list of preprocessing methods together with their parameters can be found [here](https://pytorch.org/vision/stable/transforms.html#v2-api-ref)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b7f28-eb9c-43f9-9fb2-5d5f3efabc37",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535ad82-2fbe-4160-ae4d-88051e0e4a79",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Image preprocessing and augmentation are critical steps in preparing and enhancing datasets for training machine learning models, especially in computer vision tasks.\n",
    "\n",
    "Preprocessing involves transforming raw images into a format suitable for machine learning algorithms. This step includes tasks like resizing, normalization, and noise reduction. Proper preprocessing ensures that the data is standardized, making it easier for models to learn patterns and features effectively. Normalizing pixel values, for example, by scaling them to a certain range, helps models converge faster during training by reducing the effect of varying pixel intensities.\n",
    "\n",
    "Augmenting the dataset through techniques like rotation, flipping, and cropping generates additional training examples, thereby improving model robustness and generalization. By presenting modified versions of images, augmentation helps models learn invariant features and become more tolerant to variations in the input data. It reduces overfitting by exposing the model to a wider range of scenarios and variations that might occur in real-world data.\n",
    "\n",
    "The combined impact of preprocessing and augmentation is significant. Preprocessing ensures data uniformity and consistency, making it easier for models to learn, while augmentation expands the dataset's diversity, making models more adaptable to real-world complexities. These steps collectively contribute to enhancing a model's performance, accuracy, and ability to handle unseen variations, leading to more reliable and robust AI systems in various applications like object detection, image classification, and segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe145ac-7ca1-4a12-9c9c-cfa9d2448ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

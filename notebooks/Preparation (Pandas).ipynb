{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5229404c-282d-41c6-bed6-384e49111b88",
   "metadata": {},
   "source": [
    "<img src=\"data/images/div/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195f585",
   "metadata": {},
   "source": [
    "# Preparation: Pandas\n",
    "\n",
    "Structured data typically comes in form of a matrix or table with rows representing individual data items and columns the attributes (or features) of a data item. For this kind of table-like data, the [`pandas`](https://pandas.pydata.org/) library represents a versatile and powerful data analysis tool for Python.\n",
    "\n",
    "The `pandas` library is a powerful and widely used open-source data manipulation and analysis tool for Python. It provides data structures and functions specifically designed to handle structured data, making it an essential tool for data cleaning, exploration, and analysis. Here are the key purposes and benefits of the pandas package:\n",
    "\n",
    "* **Tabular data handling:** The primary purpose of pandas is to handle structured data in the form of tables or data frames. It offers the DataFrame data structure, which is a two-dimensional labeled data structure with columns of potentially different data types. This allows for efficient storage, retrieval, and manipulation of structured data, similar to working with spreadsheets or SQL tables. Pandas supports importing and exporting data from various file formats, including CSV, Excel, SQL databases, and more, making it easy to read and write data from different sources.\n",
    "\n",
    "* **Data cleaning and preprocessing:** Pandas provides a wide range of functions and methods for data cleaning and preprocessing tasks. It allows for handling missing data, data type conversion, data normalization, deduplication, reshaping, merging, and joining data sets, and much more. These functionalities enable users to transform and clean their data efficiently, ensuring it is in the desired format for subsequent analysis or modeling tasks.\n",
    "\n",
    "* **Data exploration and analysis:** Pandas offers extensive tools for data exploration, analysis, and manipulation. It provides functionalities for filtering and selecting data, sorting, grouping, aggregating, and summarizing data, performing statistical calculations, computing descriptive statistics, and visualizing data using built-in plotting capabilities. These features enable users to gain insights from their data, discover patterns, and conduct exploratory data analysis, empowering informed decision-making.\n",
    "\n",
    "* **Integration with other libraries:** Pandas seamlessly integrates with other libraries in the Python ecosystem, such as NumPy, Matplotlib, scikit-learn, and more. It serves as a bridge between data manipulation and analysis tools, allowing for seamless interoperability. This integration enables users to leverage the functionalities of other libraries alongside pandas, creating a powerful environment for data analysis, machine learning, and scientific computing.\n",
    "\n",
    "* **Efficiency and performance:** Pandas is designed to efficiently handle large datasets and optimize performance. It utilizes vectorized operations and optimized algorithms, implemented in highly efficient C or Cython code, to achieve faster execution times. Additionally, pandas provides various mechanisms for parallelization and optimized memory usage, enabling users to work with large datasets without sacrificing performance.\n",
    "\n",
    "In summary, the pandas package offers a comprehensive set of tools for data manipulation, cleaning, exploration, and analysis. Its tabular data structures, extensive functionality, integration with other libraries, and focus on performance make it a valuable asset for data scientists, analysts, and researchers working with structured data in Python. Pandas simplifies the data processing workflow, accelerates data analysis tasks, and enables efficient data-driven decision-making.\n",
    "\n",
    "Throughout this notebook, we use the publicly available [Titanic Dataset](https://www.kaggle.com/c/titanic) for the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49897e66-a41a-44b4-b7c8-bdce3d389a56",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b27a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4cec2-cbb5-4190-ae9d-3359d9bc4614",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a81c0f",
   "metadata": {},
   "source": [
    "## Reading Files\n",
    "\n",
    "The most important concept in pandas is the [`DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) which wraps the table-like data an provides all the methods for manipulating and analyzing the data. A common way to create a DataFrame is via reading files, e.g., files with comma-separated (CSV) or tab-separated (TSV) values.\n",
    "\n",
    "The method [`pandas.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) makes this very easy. It is very configurable with a large number of input parameters to fit the structure of the input file. The Titanic Dataset comes as as normal CSV file, so reading to file into a DataFrame is a breeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94efa701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/datasets/titanic/titanic.csv', index_col=False, sep=',')\n",
    "\n",
    "# Shows the first 5 rows\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc90d2-865a-4834-8697-93cc82f692c9",
   "metadata": {},
   "source": [
    "The Titanic dataset is a popular dataset used in data mining and machine learning. It provides information about the passengers aboard the RMS Titanic, which famously sank after hitting an iceberg on its maiden voyage in 1912. The dataset is often used for predictive modeling tasks, such as predicting whether a passenger survived or not based on various features. The Titanic dataset typically includes the following information for each passenger:\n",
    "\n",
    "* `PassengerId`: A unique identifier for each passenger.\n",
    "* `Survived`: Whether the passenger survived (1) or not (0).\n",
    "* `Pclass`: The passenger's class (1st, 2nd, or 3rd).\n",
    "* `Name`: The passenger's name.\n",
    "* `Sex`: The passenger's gender.\n",
    "* `Age`: The passenger's age.\n",
    "* `SibSp`: The number of siblings/spouses aboard the Titanic.\n",
    "* `Parch`: The number of parents/children aboard the Titanic.\n",
    "* `Ticket`: The ticket number.\n",
    "* `Fare`: The fare paid for the ticket.\n",
    "* `Cabin`: The cabin number.\n",
    "* `Embarked`: The port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).\n",
    "\n",
    "The goal of analyzing the Titanic dataset is typically to build a model that can predict the survival outcome (survived or not) based on the available features. This involves data preprocessing, feature engineering, model training, and evaluation. The dataset is often used to introduce concepts in data mining and machine learning due to its relatively small size and clear target variable.\n",
    "\n",
    "The Titanic dataset serves as a practical and illustrative example for understanding data mining techniques, including data exploration, feature selection, handling missing values, encoding categorical variables, model building, and evaluation. It has become a common starting point for beginners in the field to gain hands-on experience with data analysis and predictive modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3ea0d-31ce-4cd6-95c3-a0267f39b52e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640cebfb",
   "metadata": {},
   "source": [
    "## Basic Insights\n",
    "\n",
    "The first step in data mining is typically to get to know the data to be analyzed. This task is called Exploratory Data Analysis (EDA) -- you can check out the dedicated EDA notebook for more details. `pandas` provides many methods to help with performing EDA. Let's have a look at a couple of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b37d6",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "\n",
    "When creating a DataFrame from an input file, `pandas` tries to infer the data type of each attribute. The information about the attributes' data types are accessible via [`pandas.DataFrame.dtypes`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html). pandas supports a  [`list of data types`](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#basics-dtypes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be0712",
   "metadata": {},
   "source": [
    "An alternative to check the derived data types is to call [`pandas.DataFrame.info`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) to get a summary of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d961cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9aa64",
   "metadata": {},
   "source": [
    "As you notice, many attributes have been assigned the data type `object` which represents text (i.e., strings) or mixed numeric and non-numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014bf53c",
   "metadata": {},
   "source": [
    "### Basic Statistics\n",
    "\n",
    "The method [`pandas.DataFrame.describe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) provides basic statistics (incl. count, mean, standard deviation, min/max value) for all numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de51fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8031b36",
   "metadata": {},
   "source": [
    "**Important:** Note that this information is not necessarily meaningful. For example, `PassengerId`, although a number, is a categorical attribute (i.e., just a unique label for each passenger), so calculating the mean does not make sense. Knowing the type of an attribute -- here, type refers to categorical, ordinal, interval or ratio -- requires semantic understanding of the data that `pandas` if course does not have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35a190-15e8-422b-bac2-6b741d0ffba9",
   "metadata": {},
   "source": [
    "### Visalization\n",
    "\n",
    "`pandas` also comes with a variety of in-built methods to visualize the data, and integrates well with libraries such as `matplotlib`. This is very useful for EDA.\n",
    "\n",
    "For example, in the code cell below, we can have a look at the distribtion of the values for `AGE` using the method [`hist()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html) (histogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7112fc4-4242-432b-8571-acb3e1877214",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "df.hist(column='Age', bins=50)\n",
    "plt.title('Histogram of \"Age\"')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb5355-54e2-482e-bb4d-c4bf07d8fdac",
   "metadata": {},
   "source": [
    "Instead of visualizing the raw data, we can also plot DataFrames derived from the raw data -- we cover the concept of querying DataFrames further below in the notebook. Let's assume we want to plot how many passengers embarked from the different ports using a pie chart. We can accomplish this by grouping and counting the values for `Embarked` and use an in-built method of `pandas` to plot the pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e36d6-c0b3-43fd-b151-3ea428d5aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "df.groupby(['Embarked'])['Embarked'].count().plot.pie(autopct='%1.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064f4f89-84f3-4563-9d05-83a6c778d1ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a30822",
   "metadata": {},
   "source": [
    "## Selecting Data from a Pandas Dataframe\n",
    "\n",
    "It is very common that not all the data in a DataFrame is relevant for any subsequent analysis. `pandas` supports a wide range of methods for [indexing and selecting data](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html). In the following, we cover some of the more commonly used methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81e389",
   "metadata": {},
   "source": [
    "### Selecting a Single Column\n",
    "\n",
    "If the columns in the DataFrame have names, we can use these names to access the corresponding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98185fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#age = df['Age'] # Same effect\n",
    "age = df.Age\n",
    "\n",
    "age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dec042",
   "metadata": {},
   "source": [
    "A single column is [`pandas.Series`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(age))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817c051",
   "metadata": {},
   "source": [
    "The columns of a DataFrame do not have to be named. For example, not every CSV file contains a headliner with the attribute/column names. In this case, the columns can be indexed using their position in the DataFrame, similar Python lists or NumPy arrays. Integer-based indexing requires [`pandas.DataFrame.iloc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = df.iloc[:,5] # Age is the 6th column (the columns are 0-indexed)\n",
    "\n",
    "age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a63f78",
   "metadata": {},
   "source": [
    "### Selecting Multiple Columns\n",
    "\n",
    "Instead of selecting a single column one can also select multiple columns by specifying a list of column names. The return value will be a new DataFrame (i.e., not a Series) containing only the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c49aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age_class = df[['Age', 'Pclass']]\n",
    "\n",
    "df_age_class.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846ead4",
   "metadata": {},
   "source": [
    "Again one can use `iloc` to achieve the same result which is particularly needed if the columns have no names because the csv file did not come with any header. Since we want to index to columns -- that is, the second dimension of the DataFrame, we need splicing to cover all indexes of the first dimension -- that is, all the rows in the DataFrame, similar to indexing for NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3723c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age_class = df.iloc[:,[5,2]]\n",
    "\n",
    "df_age_class.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4553b9b",
   "metadata": {},
   "source": [
    "**Side note:** The method of selecting multiple attributes can also be used to reorder the columns. Note that in the original data, `Pclass` comes before `Age`, but we switch the order during the selection process.\n",
    "\n",
    "Instead of specifying all attributes that should form a new DataFrame, there is also the alternative to specify all attributes that should be removed from the DataFrame. This is of course more convenient when a DataFrame contains a larger number of attributes and only a small number of them are not used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d22af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_age_class = df.drop(columns =['Age', 'Pclass'])\n",
    "\n",
    "df_no_age_class.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543fb3e0",
   "metadata": {},
   "source": [
    "Again, here is the alternative way using the column indices instead of the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a752a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns = [2, 5]\n",
    "\n",
    "df_no_age_class = df.drop(columns=df.columns[dropped_columns])\n",
    "\n",
    "df_no_age_class.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23809e5",
   "metadata": {},
   "source": [
    "### Selecting Rows\n",
    "\n",
    "\n",
    "#### Selecting Rows via Indexing\n",
    "\n",
    "The most obvious way to select or multiple rows is via indexing. However, the indexing is not quite as intuitive as it is not fully consistent with Python indexing convention. One the one hand, getting, e.g., the first 3 rows works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df[0:3]\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aa362b",
   "metadata": {},
   "source": [
    "However, trying the obvious way to select a single row will result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f703a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b6436",
   "metadata": {},
   "source": [
    "The correct way is again to use `iloc` to explicitly specify indexing using integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.iloc[[0]]\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7662e",
   "metadata": {},
   "source": [
    "Of course, `iloc` also supports splices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ffae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.iloc[0:3]\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd400e9e",
   "metadata": {},
   "source": [
    "#### Selecting Rows via Boolean Indexing\n",
    "\n",
    "Much more useful are methods to select rows based on filter conditions, typically by specifying the values of attributes. For example, let's get all minors, i.e., all passengers of age 17 or younger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997eccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Age < 18].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a743c",
   "metadata": {},
   "source": [
    "We can of course combine multiple criteria. For example, let's get all first class passengers younger than 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed68624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.Age < 18) & (df.Pclass == 1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596a4b4",
   "metadata": {},
   "source": [
    "Using [`isin()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isin.html#pandas.Series.isin) we can also specify a list of valid values for an attribute. The example below extracts all passengers of age 20, 30, or 40 -- probably not a really meaningful selection but it just for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131514de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Age.isin([20,30,40])].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f80b55",
   "metadata": {},
   "source": [
    "`pandas` also comes with a wider range of boolean methods that allow for indexing and selecting rows based on attribute values. For example, [`pandas.Series.str.contains`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html) checks if pattern or regex is contained within a string. These can be simple substring patterns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86248e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Name.str.contains('Miss.')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f883ef",
   "metadata": {},
   "source": [
    "...or arbitrarily complex regex patterns. The example below selects all passengers that have book 2 cabins on Level C. Note that this might not be the best regex to accomplish this but it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b26c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Cabin.str.contains(r'^C[0-9\\s]*C[0-9]*$', na=False)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767eb9a-92ee-4e16-b964-ac37edb9b94d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35747229",
   "metadata": {},
   "source": [
    "## Querying DataFrames\n",
    "\n",
    "Selecting rows based on attributes values is already a form of querying a DataFrame -- analogous to basic `WHERE` clauses in SQL queries. However, `pandas` also supports more advanced methods that mimic grouping and aggregating similar to SQL queries.\n",
    "\n",
    "### Basic Aggregation Operations\n",
    "\n",
    "**Count -- Number of Attribute Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71758c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a601506",
   "metadata": {},
   "source": [
    "**Minimum and Maximum Value of an Attribute**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa676059",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993e049",
   "metadata": {},
   "source": [
    "Note that this also works for string values since strings can be sorted lexicographically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e73ad",
   "metadata": {},
   "source": [
    "**Summing all Attribute Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8265343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Fare.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Name.sum() # This works as well: summing up strings means concatenating all strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06ab4f",
   "metadata": {},
   "source": [
    "**Mean and Standard Deviation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c48da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f79ae",
   "metadata": {},
   "source": [
    "The mean and standard deviation are of course only defined over numerical values and these two methods will throw an error when applied to a string attribute.\n",
    "\n",
    "Naturally, this aggregation method can be used in combination with different filter conditions. For example, we can compare the mean ages and the standard deviations for 1st, 2nd, and 3rd class passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342673ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [1, 2, 3]:\n",
    "    df_class = df[df.Pclass == c]\n",
    "    print(\"Class {}: Mean={:.2f}, StdDev={:.2f}\".format(c, df_class.Age.mean(), df_class.Age.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a46f39",
   "metadata": {},
   "source": [
    "### Grouping & Aggregation\n",
    "\n",
    "Basic aggregation works across all data samples, and when we want to perform aggregation on subsets, we can use multiple independent aggregation queries as a default solution -- see the example above for calculating the mean and standard deviation for each passenger class. However, these independent queries involve repeating the same processing steps several times. For example, we have to go over the DataFrame above 3 times to extract all passengers of the same class. Particularly for large datasets with many required queries, this can significantly affect the overall performance in a negative way.\n",
    "\n",
    "But again, `pandas` got us cover and supported grouping, i.e., the specification of subsets of the data over which aggregation methods applied (similar to `GROUP BY` statements in SQL queries). Using grouping, we can calculate the means and standard deviation for all 3 passenger classes using a single query.\n",
    "\n",
    "When applying only one aggregation function, the syntax is pretty simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Pclass')['Age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31d994",
   "metadata": {},
   "source": [
    "When applying multiple aggregation functions over the same groups, the most practical approach is via \"named aggregation\" where you define a name of each resulting aggregation that will form your result DataFrame. Using named aggregation we can get the means and standard deviations using a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d42fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Pclass']).agg(mean_Age=('Age', 'mean'), std_Age=('Age', 'std'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216edf7",
   "metadata": {},
   "source": [
    "As one would expect, we can define groups over more than one attribute at a time. Let's extend our example to form groups with respect to the age as well as whether a passenger has survived or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c96928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Pclass', 'Survived']).agg(mean_Age=('Age', 'mean'), std_Age=('Age', 'std'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7931250-5a97-4cc4-aa9d-fceb171d1465",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91913f2e",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac6c97",
   "metadata": {},
   "source": [
    "Apart from just querying a DataFrame to analyze the data, `pandas` also provides many methods to manipulate the data as part of preprocessing for subsequent analyses. Data manipulation can refer to changing existing values (e.g., rounding, scaling, normalizing) but also by creating new columns/features, typically derived from existing feature values. Data preprocessing is a topic of the first lecture, and there will be a dedicated lecture notebook that performs common preprocessing steps using `pandas`. Hence, we can skip a detailed discussion here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abff09-458a-47ae-a806-ac58af3281a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d388170",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Pandas is a powerful Python library for data mining. Its purpose is to handle structured data, providing efficient data manipulation and preprocessing capabilities. It simplifies tasks like data cleaning, exploration, and analysis, offering functions for filtering, sorting, grouping, and summarizing data. Pandas seamlessly integrates with other libraries, enabling interoperability and extending its functionality. Its versatility and user-friendly interface make it a go-to tool for extracting insights from data.\n",
    "\n",
    "A good grasp of the methods provided by pandas can make life very easy as many queries and preprocessing steps can be implemented using very few lines of code. Similar to NumPy -- in fact, Pandas internally relies on NumPy -- the underlying algorithms are not implemented in Python but in C/C++ making the execution typically much faster compared to the runtime of the same algorithms implemented in native Python. This notebook and the EDA notebook cover only a small subset of the feature set of pandas, and it is worthwhile to check the documentation in case for more specific task when handling your own datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699baf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

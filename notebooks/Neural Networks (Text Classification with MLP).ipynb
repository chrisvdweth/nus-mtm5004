{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce60e7d-4d2c-4101-a45b-356606be54fe",
   "metadata": {},
   "source": [
    "<img src=\"data/images/div/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19d0a9-13f7-408c-82f8-ae7dbef28125",
   "metadata": {},
   "source": [
    "# Neural Networks (MLP)\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is one of the most common neural network models used in the field of deep learning. Often referred to as a \"vanilla\" neural network, an MLP is simpler than the complex models of today's era (e.g., CNN, RNN, Transformer). However, the techniques it introduced have paved the way for further advanced neural networks. An MLP consists of interconnected neurons (i.e., Logistic Regression units) transferring information to each other. \n",
    "\n",
    "The MLP is a feedforward neural network, which means that the data is transmitted from the input layer to the output layer in the forward direction. All neurons of one layer are connected to all the neurons in the next layer. The connections between the layers are assigned weights. The weight of a connection specifies its importance. This concept is the backbone of an MLP's learning process.\n",
    "\n",
    "**Important:** This notebook does not serve as a proper introductory tutorial into PyTorch, it only provides a minimal example for implementing a basic MLP text classifier, glossing over many details. If you are totally new to PyTorch, it's highly recommended to check out the tutorials on the [official PyTorch page](https://pytorch.org/tutorials/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767575d-c9d2-495f-88c8-5e9a0c5947d4",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bab8a-49d2-46a4-b17f-79dcb5da5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ee3e4-6563-4cb6-98ae-8a507fe3de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40e1d9-b2a2-4fcf-a066-893e0b5fd4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a599743-2bdd-4621-b359-fa690cf59e5b",
   "metadata": {},
   "source": [
    "### Checking/Setting the Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPU to significantly speed up the training process. If you have a support GPU, feel free to utilize it. However, for this notebook it's certainly not needed as our dataset is small and our network model is very simple. In fact, the training is fast on the CPU here since initializing memory on the GPU and moving the data to the GPU involves some overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abac52a-5539-4b7b-9681-6910031304c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU (in case you don't have a supported GPU)\n",
    "# With this small dataset and simple model you won't see a difference anyway\n",
    "use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95605cb-d78c-4176-aff3-89f787d4c559",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "For this notebook, we use a simple dataset for sentiment classification. This dataset consists of 10,662 sentences, where 50% of the sentences are labeled 1 (positive), and 50% of the sentences are labeled -1 (negative).\n",
    "\n",
    "### Loading Sentence/Label Pairs from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4014a-4e71-4705-b448-35ed3ffb22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = [], []\n",
    "\n",
    "with open(\"data/datasets/sentence-polarities/sentence-polarities.csv\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        sentence, label = line.split(\"\\t\")\n",
    "        sentences.append(sentence)\n",
    "        labels.append(int((int(label)+1)/2))\n",
    "        \n",
    "print(\"Total number of sentences: {}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728c8fc-19f5-46b8-b4ae-c5020a54c0d4",
   "metadata": {},
   "source": [
    "### Create Training & Test Set\n",
    "\n",
    "To evaluate any classifier, we need to split our dataset into a training and a test set. With the method `train_test_split()` this is very easy to do; this method also shuffles the dataset by default, which is important for this example, since the dataset file is ordered with all positive sentences coming first. In the example below, we set the size of the test set to 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8e763-c7f7-44db-ab32-e799eab2ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentences and labels into training and test set with a test set size of 20%\n",
    "sentences_train, sentences_test, labels_train, labels_test = train_test_split(sentences, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# We can directly convert the numerical class labels from lists to numpy arrays\n",
    "y_train = np.asarray(labels_train, dtype=np.int8)\n",
    "y_test = np.asarray(labels_test, dtype=np.int8)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(sentences_train)))\n",
    "print(\"Size of test set: {}\".format(len(sentences_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5afb3b-55fb-4b5d-84e0-95f95d299f5f",
   "metadata": {},
   "source": [
    "### TF-IDF Feature Extraction\n",
    "\n",
    "To serve as valid input for our network, we need to convert our sentences into document vectors. For this, we use as always a scikit-learn vectorizer. In the code cell below, you can try the TF-IDF vectorizer or the Count vectorizer. The final results will be more or less the same. Still, feel free to play with the choice of the vectorizer as well as with the different input parameters (e.g., `ngram_range` or `max_features`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1edf0-b6c7-46d1-b55e-6e98ce04b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document-Term Matrix for differen n-gram sizes\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "#vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "\n",
    "X_train = vectorizer.fit_transform(sentences_train)\n",
    "X_test = vectorizer.transform(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e95375-0b9f-4af2-9408-eaac626bea26",
   "metadata": {},
   "source": [
    "### Create Tensors\n",
    "\n",
    "Both `X_train` and `X_test` are now our matrices containing the document vectors of our training and test set. However, right now, `X_train` and `X_test` are sparse matrices, i.e., representations that only store the non-zero values. Further use with PyTorch, we have to perform 2 additional steps:\n",
    "\n",
    "* Convert the sparse representation to a dense (i.e., full/normal) representation using `.todense()`; the output will be numpy arrays\n",
    "\n",
    "* Convert numpy arrays to tensors. `Tensor` is the data object used by PyTorch; they look, feel, and handle basically the same as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc34ea-1b43-4509-869b-6321c4de8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default Tensor stores float values\n",
    "X_train = torch.Tensor(X_train.todense())\n",
    "X_test = torch.Tensor(X_test.todense())\n",
    "\n",
    "# Our labels are integers, hence we use LongTensor\n",
    "# (that's required, otherwise we would get an error later)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329bccd-540b-4528-ac3c-1e50bfa08435",
   "metadata": {},
   "source": [
    "### Create PyTorch Datasets\n",
    "\n",
    "Training a neural network is usually not done computing the gradient w.r.t. the whole dataset as most of the time the dataset is way too large to fit into memory. It might also slow down training since the gradients w.r.t. the whole dataset can be very small (although this could be addressed by increasing the learning rate). In practice, the training is basically always done using batches, i.e., much smaller subset of the data. While we can take `X_train` and `X_test` and implement our own loops to create batches, PyTorch comes with a series of convenient utility classes to simplify things.\n",
    "\n",
    "The first utility class we use is the [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class; more specifically, since `Dataset` is in abstract class, we use the [`TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) class to wrap our tensors and make each sample retrievable by indexing the tensors along the first dimension. This will be used by the data loaders below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94153c94-821c-4118-b458-f3ff683f1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "dataset_test = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db00cd0-1bec-4b2f-8675-997c6623f676",
   "metadata": {},
   "source": [
    "### Create Data Loaders\n",
    "\n",
    "The [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class takes a `DataSet` object as input to handle to split the dataset into batches. As such, a data loader also has `batch_size` as an input parameter. In the following, we use a batch size of 64, although you can easily go higher since we are dealing with only sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ba17a-8c1a-4a6f-b32e-8fc393c7d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88334763-e55e-490b-a416-e831cff94642",
   "metadata": {},
   "source": [
    "We can use the data loaders to loop over all batches and use them for training and testing our model. The code cell below shows the general idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc07d8-01d4-40cb-875d-2b6921263946",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, y_batch in loader_train:\n",
    "    print(X_batch.shape)\n",
    "    print(y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce34b2-499c-4712-a4d0-2f9092652823",
   "metadata": {},
   "source": [
    "Appreciate how much additional code we would have written if we had implemented this batching on our own. Well, it actually wouldn't be that much, but using these utility classes makes our code much cleaner and less prone to error. These utility classes also allow to train models in a distributed setting; but this is not important at the moment.\n",
    "\n",
    "With respect to preparing our data, we are not ready to build and train a neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb06d21-409f-4154-94d5-671e9600f2f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460f9e6-7c78-4bbb-b315-560d3b855ef5",
   "metadata": {},
   "source": [
    "## Build Network\n",
    "\n",
    "In this notebook, we replicate the exact model architecture we used as an example on the lecture slides: We have 3 hidden layers and 2 outputs. Note that we could implement the model with just 1 output since we are trying to solve binary classification tasks. However, using 1 output and the Binary Cross Entropy Loss or using 2 outputs and using the Multiclass Cross Entropy Loss is completely equivalent. Hence, we simple implement to model exactly like visualized below:\n",
    "\n",
    "<img src=\"data/images/examples/example-network-mlp.png\">\n",
    "\n",
    "Of course, our input layer doesn't have just 3 features but 10,000 (assuming that's the value of `max_features` of the vectorizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc5a57-cca2-426f-9d42-6ba61a05194e",
   "metadata": {},
   "source": [
    "### Basic Implementation\n",
    "\n",
    "The code cell below shows the most straightforward implementation of our network architecture. To make the model a bit flexible, we have the `vocab_size` (vocabulary size) as an input parameter. This means, any time we change the `max_features` parameter of the vectorizer, we define our model using the value. In this implementation, we use [`nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) (Rectified Linear Unit) as the activation function for each unit, which became often of the activation of choice. Still, fill free to try out other activation functions (e.g., [`nn.Tanh`](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh)).\n",
    "\n",
    "For the activation function for the output layer we use [`nn.LogSoftmax`](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax) -- instead of [`nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax) -- which applies $\\log(Softmax(x))$ to its input. This is very common in practice as it makes computation typically numerical stable as Softmax probabilities can be very small, particularly in case of many class labels. While we have only 2 classes here, and Softmax will probably do just fine, using the LogSoftmax is just a good practice. Of course, the outputs are no longer probabilities but log probabilities. This will affect the choice of the loss functions; see below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9896599-2cac-445f-a407-77ef48702d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet1(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # Define 1st fully connected (i.e., linear) hidden layer\n",
    "        self.fc1 = nn.Linear(self.vocab_size, 4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Define 2st fully connected (i.e., linear) hidden layer\n",
    "        self.fc2 = nn.Linear(4, 3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Define 3st fully connected (i.e., linear) hidden layer\n",
    "        self.fc3 = nn.Linear(3, 3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # Define output layer (which is also a linear layer)\n",
    "        self.out = nn.Linear(3, 2)        \n",
    "        # Define log softmax layer\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.fc1(X)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.out(out)\n",
    "        log_probs = self.log_softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b6a4d-2c1e-4acc-81e4-57766a05b06b",
   "metadata": {},
   "source": [
    "To \"visualize\" the network, we can create and print an instance of the class `SimpleNet1`. The output should reflect the model architecture shown in the image above.\n",
    "\n",
    "The command `.to(device)` \"moves\" the instance to the selected instance (e.g., the CPU or GPU). In general, both the model and the data need to reside on the same instance. So if the model will be on the GPU, we also need to move the data later to the GPU. Using consistently `.to(device)` on the model and the data (see below) ensures that there will be no mismatch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c479be4-8253-4e39-ab04-cd4bad733583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move to device\n",
    "classifier = SimpleNet1(X_train.shape[1]).to(device)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fefc765-1aae-4a33-819b-b8aacb71ade5",
   "metadata": {},
   "source": [
    "### Modified Implementation\n",
    "\n",
    "In the implementation above, we used the most basic approach by having 2 main parts:\n",
    "\n",
    "* Defining all the layers in the `__init__()` method\n",
    "\n",
    "* Push the data through all layers in the `forward()` method\n",
    "\n",
    "While this is perfectly fine and for more complex models essentially required, here we can use some additional utility classes to simplify our code. Note how we push our data step-by-step (i.e., sequentially) through each layer. This means we can create a model component using [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) that contains all our layers -- here, the model component is basically the complete model. Thus, in the `forward()` method, we only need to give the component the data, and the component will automatically push the data through all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec227eb-dccc-4ac3-a0f3-52adf980834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet2(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.vocab_size, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        log_probs = self.net(X)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660c6ed-535f-43e3-a6d0-43c04ff7b3dd",
   "metadata": {},
   "source": [
    "This is a neat way to save lines of code and make the implementation more readable and maintainable. Of course, if we create and print an instance of this class, the output should match the one frome `SimpleNet1` with respect to the core layers. Since we use `nn.Sequential` here, the output is not exactly the same, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a433594f-213e-4754-951f-e660d6949282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move to device\n",
    "classifier = SimpleNet2(X_train.shape[1]).to(device)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccedd111-80c7-48ed-85fa-dec417203ed9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcdfc95-3717-444e-a5fe-1e8b353aa2b0",
   "metadata": {},
   "source": [
    "## Train & Evaluate Model\n",
    "\n",
    "With the data prepared and the model architecture defined, we can now train and evaluate a model to build our sentiment classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ea92d-8c40-414f-98bb-d9ceb36ffc47",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "The code cell below implements the method `evaluate()` to, well, evaluate our model. Apart from the model itself, the method also receives the data loader as input parameter. This allows us later to use both `loader_train` and `loader_test` to evaluate the training and test loss using the same method.\n",
    "\n",
    "The method is very generic and is not specific to the dataset. It simply loops over all batches of the data loader, computes the log probabilities, uses these log probabilities to derive the predicted class labels, and compares the predictions with the ground truth to return the f1 score. This means, this method could be used \"as is\" or easily be adopted for all kinds of classifications tasks (incl. task with more than 2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89c567-bba3-4bcf-8a04-22000dd31cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "\n",
    "    # Set model to \"eval\" mode (not needed here, but a good practice)\n",
    "    model.eval()\n",
    "\n",
    "    # Collect predictions and ground truth for all samples across all batches\n",
    "    y_pred, y_test = [], []\n",
    "\n",
    "    with tqdm(total=len(loader)) as pbar:\n",
    "        \n",
    "        # Loop over each batch in the data loader\n",
    "        for X_batch, y_batch in loader:\n",
    "\n",
    "            # Move data to device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Push batch through network to get log probabilities for each sample in batch\n",
    "            log_probs = model(X_batch)                \n",
    "            \n",
    "            # The predicted labels are the index of the higest log probability (for each sample)\n",
    "            y_batch_pred = torch.argmax(log_probs, dim=1)\n",
    "\n",
    "            # Add predictions and ground truth for current batch\n",
    "            y_test += list(y_batch.detach().cpu())\n",
    "            y_pred += list(y_batch_pred.detach().cpu())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Set model to \"train\" mode (not needed here, but a good practice)\n",
    "    model.train()            \n",
    "            \n",
    "    # Return the f1 score as the output result\n",
    "    return metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5466f6-7e3a-498c-9caf-d9748c3eea72",
   "metadata": {},
   "source": [
    "For a quick test, let's evaluate the newly created model. Of course, we didn't train our model, but it will still make predictions based on the initial weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5d753-ee64-4489-84fc-14d68146f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(classifier, loader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441710d-8833-4975-b5df-47942ab1f3f1",
   "metadata": {},
   "source": [
    "### Train Model (and evaluate after each epoch)\n",
    "\n",
    "Similar to the method `evaluate()` we also implement a method `train()` to wrap all the required steps training. This has the advantage that we can simply call `train()` multiple times to proceed with the training. Apart from the model, this method has the following input parameters:\n",
    "\n",
    "* `loader_train` and `loader_test`: this allows us to compute the f1 score over the training data an the test data after each epoch; we can later visualize the changes in the f1 scores\n",
    "\n",
    "* `optimizer`: the optimizer specifier how the computed gradients are used to updates the weights; in the lecture, we only covered the basic Stochastic Gradient Descent, but there are much more efficient alternatives available\n",
    "\n",
    "* `criterion`: this is the loss function; \"criterion\" is just very common terminology in the PyTorch documentation and tutorials\n",
    "\n",
    "* `num_epochs`: the number of epochs -- i.e., the number of times we want train over all samples in our dataset\n",
    "\n",
    "The heart of the method is the snippet described as PyTorch Magic. It consists of the following 3 lines of code\n",
    "\n",
    "* `optimizer.zero_grad()`: After each training step for a batch if have to set the gradients back to zero for the next batch\n",
    "\n",
    "* `loss.backward()`: Calculating all gradients using backpropagation\n",
    "\n",
    "* `optimizer.step()`: Update all weights using the gradients and the method of the specific optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63a2d1-ddca-453f-9290-f6fc0d93233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader_train, loader_test, optimizer, criterion, num_epochs):\n",
    "\n",
    "    losses, f1_train, f1_test = [], [], []\n",
    "    \n",
    "    # Set model to \"train\" mode (not needed here, but a good practice)\n",
    "    model.train()\n",
    "\n",
    "    # Run all epochs\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        # Initialize epoch loss (cummulative loss fo all batchs)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        with tqdm(total=len(loader_train)) as pbar:\n",
    "\n",
    "            # Loop over each batch in the data loader\n",
    "            for X_batch, y_batch in loader_train:\n",
    "\n",
    "                # Move data to device\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                # Push batch through network to get log probabilities for each sample in batch\n",
    "                log_probs = classifier(X_batch)                \n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(log_probs, y_batch)\n",
    "\n",
    "                ### PyTorch Magic! ###\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Keep track of overall epoch loss\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Keep track of all epoch losses\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        # Compute f1 score for both TRAINING and TEST data\n",
    "        f1_tr = evaluate(model, loader_train)\n",
    "        f1_te = evaluate(model, loader_test)\n",
    "        f1_train.append(f1_tr)\n",
    "        f1_test.append(f1_te)\n",
    "\n",
    "        print(\"Loss:\\t{:.3f}, f1 train: {:.3f}, f1 test: {:.3f} (epoch {})\".format(epoch_loss, f1_tr, f1_te, epoch))\n",
    "     \n",
    "    # Return all losses and f1 scores (all = for each epoch)\n",
    "    return losses, f1_train, f1_test        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0924b60-0244-475f-a14a-7ffea9b4500d",
   "metadata": {},
   "source": [
    "Before we can actually train the model, we need to instantiate the `criterion` (i.e., the loss function) and the `optimizer`. Since out model returns log probabilities, we need to use the [`nn.NLLLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html), i.e., the Negative Log Likelihood Loss. This is basically the same as the Multiclass Cross Entropy Loss but using log probabilities instead of probabilities.\n",
    "\n",
    "For the optimizer, we pick the widely used [`Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer. Adam optimizer is the extended version of stochastic gradient descent which could be implemented in various deep learning applications such as computer vision and natural language processing in the future years. Adam was first introduced in 2014. The name is derived from adaptive moment estimation. The optimizer is called Adam because it uses estimations of the first and second moments of the gradient to adapt the learning rate for each weight of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230ef76-9592-4e32-b5e4-5cb97aa9241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and movie to device\n",
    "classifier = SimpleNet2(X_train.shape[1]).to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Define optimizer (you can try, but the basic Stochastic Gradient Descent (SGD) is actually not great)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56b411-f4d2-4664-83d7-2ce38b873a44",
   "metadata": {},
   "source": [
    "Now we finally have everything in place to train the model. For this, we now only need to call the `train()` in the code cell below. Note that you can run the code cell below multiple times to continue the training for further 10 epochs. Each epoch will print 3 progress bars:\n",
    "\n",
    "* training over training set\n",
    "\n",
    "* evaluating over training set\n",
    "\n",
    "* evaluating over test set\n",
    "\n",
    "After each epoch, a print statement will show the current loss as well as the latest f1 scores for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a3b09-4bcf-4006-bb8c-78d843682706",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "losses, f1_train, f1_test = train(classifier, loader_train, loader_test, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2f01d-0c96-4a7e-82eb-141ec71d9336",
   "metadata": {},
   "source": [
    "Since the method `train()` returns the losses and f1 scores for each epoch, we can use this data to visualize how the loss and the f1 scores change over time, i.e., after each epoch. The code cell below creates the corresponding plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40086674-fe58-4708-967b-b337e3a72df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, len(losses)+1))\n",
    "\n",
    "# Convert losses to numpy array\n",
    "losses = np.asarray(losses)\n",
    "# Normalize losses so they match the scale in the plot (we are only interested in the trend of the losses!)\n",
    "losses = losses/np.max(losses)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x, losses, lw=3)\n",
    "plt.plot(x, f1_train, lw=3)\n",
    "plt.plot(x, f1_test, lw=3)\n",
    "\n",
    "font_axes = {'family':'serif','color':'black','size':16}\n",
    "\n",
    "plt.gca().set_xticks(x)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontdict=font_axes)\n",
    "plt.ylabel(\"F1 Score\", fontdict=font_axes)\n",
    "plt.legend(['Loss', 'F1 (train)', 'F1 (test)'], loc='lower left', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6011403-3aeb-486e-bd86-69a87417047e",
   "metadata": {},
   "source": [
    "From the plot, we can observe several things\n",
    "\n",
    "* The loss goes down! This essentially means that our model is learning. This can be very useful as an incorrectly implemented model may not throw an error but not train properly (i.e., the loss not going down). So it's a good sanity check when implementing and using a new model.\n",
    "\n",
    "* The f1 score over the training data reaches 1.0 (at least after more epoch). This means that the model learns to correctly predict the label for all training samples. Of course, this is not what we are interested in but (a) again tells us that the model is generally learning, and (b) this might give us insights that our model is overfitting.\n",
    "\n",
    "* The f1 score over the test data almost plateaus after Epoch 4 and even tend to decrease a bit later. This indicates that the model starts to overfit after Epoch 6\n",
    "\n",
    "**Important:** This plot showing the trends for the loss and f1 scores (or other metrics) can look very different depending on the dataset, the network architecture, and the hyperparameters. While in this simple example all trends seem to smoothly converge, this is not the case in general. For example, the f1 score for the test data might see a much steeper drop after peaking, which would even more clearly indicate the model is starting to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a835d8-7e6e-4028-986b-29643e425120",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28199202-477e-4260-83af-d83a6fa251ed",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we built our first text classifier using PyTorch. In more detail, we trained and evaluated a Multi-Layer Perceptron to build a classifier for sentiment analysis. Building and training more complex architecture might require more code, but most of the essential steps have been covered here. For example, we already made use of important utility classes such as `Dataset` and `DataLoader` that help us prepare our datasets for the training using batches.\n",
    "\n",
    "Appreciate how PyTorch hides all the nitty-gritty details of the training process, most importantly the computation of all gradients and the updates of the weights. While implementing backpropagation from scratch is not too difficult for simple MLP architectures, for more advanced architectures covered later this would become quite demanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e929ff-d1af-43f6-ae45-376f9ae3e369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/div/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Topic modeling is a technique used in natural language processing (NLP) and machine learning to identify topics or themes within a collection of documents. It's an unsupervised learning method that aims to uncover hidden patterns or structures within a set of texts.\n",
    "\n",
    "The primary goal of topic modeling is to automatically analyze and extract meaningful topics from a large corpus of documents without needing prior annotations or labels. One of the most popular algorithms for topic modeling is Latent Dirichlet Allocation (LDA). LDA assumes that each document is a mixture of various topics, and each word in the document is attributable to one of those topics.\n",
    "\n",
    "Here's a simplified way it works:\n",
    "\n",
    "* **Preprocessing:** Text data is cleaned, tokenized, and prepared for analysis by removing stop words, punctuation, and other irrelevant information.\n",
    "\n",
    "* **Vectorization:** Documents are represented as numerical vectors, often using techniques like the bag-of-words model or TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "* **Topic Modeling:** Algorithms like LDA are applied to these numerical representations to identify underlying topics based on the co-occurrence of words across documents. These topics are represented as a distribution of words.\n",
    "\n",
    "* **Interpretation:** Once topics are identified, analysts or researchers interpret and label these topics based on the most frequent or representative words within each topic.\n",
    "\n",
    "Topic modeling finds applications in various fields like information retrieval, content recommendation, sentiment analysis, and understanding trends in large text datasets, helping to organize, summarize, and make sense of extensive textual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import all important packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from tqdm import tqdm\n",
    "from src.plotutil import show_wordcloud\n",
    "\n",
    "import spacy\n",
    "# Load English language model (if missing, check out: https://spacy.io/models/en)\n",
    "nlp = spacy.load('en_core_web_md')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Toy Data\n",
    "\n",
    "### Definition of Toy Dataset\n",
    "\n",
    "For this simple example, we define our corpus as a list of documents. Each document is only a single sentence to keep the example easy to follow. Naturally, a document may contain a large number of sentences. You will notice that this toy dataset includes two main topics: \"pets, cats, dogs\" and \"programming, python\". We will see how this observation will be reflected in the result later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Cats and dogs are both domesticated animals.\",\n",
    "             \"The domestication of dogs started 10,000 years ago.\",\n",
    "             \"Dogs were easier to domensticate than cats.\",\n",
    "             \"Some people have a dog and a cat (or several dogs and cat) as pets.\",\n",
    "             \"The domestication of animals was an important part of human progress.\",\n",
    "             \"Python is a programming laguage that is easy to learn\",\n",
    "             \"Python makes text processing rather easy.\",\n",
    "             \"A lot of programming languages support text analysis.\",\n",
    "             \"Programming in Python makes the analysis of text easy\",\n",
    "             \"NLTK is a great NLP package for Python.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "LDA assumes as input bags of words, not sequences of words. It also strongly benefits from normalization, as, for example, the capitalization of words, the tense of verbs, or the plurality of nouns arguably do not affect the topic of a document. Let's therefore use spaCy to form tokenization, case-folding and lemmatization. We also remove all stopwords and punctuation marks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_documents = []\n",
    "\n",
    "for doc in documents:\n",
    "    doc = nlp(doc)\n",
    "    processed_documents.append(' '.join([ t.lemma_.lower() for t in doc if t.is_stop == False and t.is_punct == False]))\n",
    "\n",
    "# Print the processed documents\n",
    "for doc in processed_documents:\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Term-Document Matrix (TDM)\n",
    "\n",
    "In practice, we often limited the number of considered words (i.e., our vocabulary) to the most frequent words. Of course, for the very small toy dataset, this is not really needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000 # Top 1000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CountVectorizer` is, among other vectorizers, a handy and flexible way to generate a document term matrix. More specifically, here each value in the matrix represents the count of how a term $t_i$ occurs in document $d_j$. In contrast to TF-IDF values, LDA requires only the word counts without any additional weights\n",
    "\n",
    "The [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) of `scikit-learn` class allows for a wide range of useful input parameters to configure the generation of the document term matrix; In this example, we use the following:\n",
    "\n",
    "- `max_df`: If not `None` one can specify how often a word has to be in the corpus AT MOST, either in relative terms or in absolute terms. This allows us to ignore words that are very COMMON across all documents and that are not very discriminative.\n",
    "- `min_df`: If not `None` one can specify how often a word has to be in the corpus AT LEAST, either in relative terms or in absolute terms. This allows us to ignore rare words that are very RARE across all documents and that are not very discriminative.\n",
    "- `max_features`: If not `None` one can limit the number of words to ones with the highest counts (term frequencies) across the whole corpus\n",
    "- `stop_words`: If not `None` one can specify the list of stop words to be removed from each document (not really necessary if stop words are removed during preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=num_words)\n",
    "\n",
    "tf_tdm = tf_vectorizer.fit_transform(processed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the vocabulary, we notice that we are missing several non stopwords. This is because we have removed all words that appear only once across all sentences (e.g., *\"year\"*, *\"processing\"*) -- notice the parameter `min_df=2` in the code cell above. Again, this is very common to ignore very rare words, even if they would be in the list of `num_words` most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = tf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Term-Document Matrix\n",
    "\n",
    "Just for illustrative purposes, let's print the term-document matrix. This is only meaningful for the toy datasets, but highlights the effects of the different preprocessing options even before performing LDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(tf_tdm.A.T, index=list(vocabulary), columns=['d{}'.format(c) for c in range(1, len(vocabulary)+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform LDA\n",
    "\n",
    "First, we need to set the number of topics. In practice, this is not known a-priori. For our toy example, we know to expect 2 main topics. You can still change the value and then interpret and compare the different results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 2\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, max_iter=100, learning_method='online', learning_offset=50.,random_state=0).fit(tf_tdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the model are not probabilities, i.e., the values do not sum up to 1. In most cases, this is not a problem since the absolute values but the relative differences are the important parts. In other words, most of the time these values do not matter at all. However, for illustrative purposes, we can normalize all values to proper probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_ /= lda.components_.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the results\n",
    "\n",
    "#### Show distribution of words for topics\n",
    "\n",
    "`display_topics()` is just a utility method to display the results. For each topic, it ranks all words with respect to their probabilities and list the top *N* words. Again, for our small toy dataset with the small vocabulary, we can easily print all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_features):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic {}\".format(topic_idx))\n",
    "        for feature_idx in topic.argsort()[:-num_top_features-1:-1]:\n",
    "            print (\"\\t{0:20} {1}\".format(feature_names[feature_idx],topic[feature_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this method to our LDA result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda, vocabulary, num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show which document belongs to which topic\n",
    "\n",
    "The method `transform()` takes as input a document-topic matrix X and returns topic distribution for X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = lda.transform(tf_tdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `display_documents()` shows the topic for each document. To this end, the method picks the topic with the highest probability. Recall that each document is a distribution over all topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_documents(document_topic_matrix, max_documents=10):\n",
    "    num_documents = document_topic_matrix.shape[0]    # Get the number of documents\n",
    "    for n in range(min(num_documents,max_documents)): # Never show more than #max_documents documents\n",
    "        topic_distribution = document_topic_matrix[n] # List of probabilities, e.g., [0.032, 0.233, 0.001, ...]\n",
    "        topic_most_pr = topic_distribution.argmax()   # Pick the list index with the highest probability\n",
    "        print(\"doc: {}   topic: {}\".format(n,topic_most_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the results for the toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_documents(doc_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic assignment should be in line with our expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize distribution of words for topics using word clouds\n",
    "\n",
    "Particularly for larger datasets and larger vocabularies, topics are best visualized using word clouds. Here, the size of words reflects their probabilities within a topic. The method `show_wordcloud()` in the file `src/plotutils.py` handles this for us. The code cell below goes through all identified topics and generates their respective word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(num_topics):\n",
    "    feature_distribution = lda.components_[topic]\n",
    "    # Create dictionary of word frequencies as input for wordcloud package\n",
    "    word_frequencies = { vocabulary[idx]:prob for idx, prob in enumerate(feature_distribution) }\n",
    "    show_wordcloud(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application use case: news article headlines\n",
    "\n",
    "In this example, we apply LDA over a list of 12,394 news article headlines from TechCrunch (https://techcrunch.com/). This dataset is publicly available on Kaggle (https://www.kaggle.com/), see the full details [here](https://www.kaggle.com/PromptCloudHQ/titles-by-techcrunch-and-venturebeat-in-2017). For convenience, we already downloaded the dataset as CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load news article headlines from CSV file\n",
    "\n",
    "As usual, we use `pandas` reading structured files like CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/datasets/techcrunch/news-article-headlines-techcrunch.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Remove rows where Title is \"NaN\" to avoid any errors later on\n",
    "df = df[pd.notnull(df['title'])]\n",
    "\n",
    "# Extract list of headline from data frame\n",
    "news_headlines = df['title'].tolist()\n",
    "\n",
    "# Print the first 5 headlines\n",
    "for idx in range(5):\n",
    "    print (news_headlines[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all Headlines\n",
    "\n",
    "As usual, we first preprocess all news article headlines by tokenizing, lemmatizing, and case-folding all words, as well as remove all stopwords and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_news_headlines = []\n",
    "\n",
    "for doc in tqdm(news_headlines):\n",
    "    doc = nlp(doc)\n",
    "    processed_news_headlines.append(' '.join([ t.lemma_ for t in doc if t.is_stop == False and t.is_punct == False]))\n",
    "    #break\n",
    "\n",
    "# Print the first 5 processed documents\n",
    "for doc in processed_news_headlines[:5]:\n",
    "    print (doc)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Term-Document Matrix\n",
    "\n",
    "The dataset is now ready to compute the term-document matrix with all the term/word counts needed to run LDA. Since we are performing the same steps as for the toy dataset, we skip a more detailed discussion here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000 # Top 1000 words\n",
    "\n",
    "tf_vectorizer_news_headlines = CountVectorizer(max_df=0.95, min_df=5, max_features=num_words, stop_words='english')\n",
    "\n",
    "tf_news_headlines = tf_vectorizer_news_headlines.fit_transform(processed_news_headlines)\n",
    "\n",
    "vocabulary_news_headlines = tf_vectorizer_news_headlines.get_feature_names_out()\n",
    "\n",
    "print(\"Size of vocabulary:\", len(vocabulary_news_headlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform LDA\n",
    "\n",
    "Since these are 12k+ documents, setting the number of topics to 2 is usually not very meaningful. There are no straightforward rules on how to set this number. A common value to start with is 20, inspect the results, and potentially repeat this step with different values.\n",
    "\n",
    "**Note:** This will now take several seconds or even minutes, but is still manageable. If you have (really) large data, it it is recommended to apply LDA first on a sample to see if all works (no errors) and if the results \"look\" meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_topics = 20\n",
    "\n",
    "lda_news_headlines = LatentDirichletAllocation(n_components=num_topics, max_iter=100, learning_method='online', learning_offset=50.,random_state=0).fit(tf_news_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results\n",
    "\n",
    "Given the dataset size, we inspect the result by directly looking at the word clouds. Again, the code cell below performs the exact same required steps for that as we have already seen for the toy dataset. The only difference is that with the larger number of topics and the much larger vocabulary, we now can plot more word clouds, and each word cloud contains more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {}\n",
    "\n",
    "for topic in range(num_topics):\n",
    "    feature_distribution = lda_news_headlines.components_[topic]\n",
    "    # Create dictionary of word frequencies as input for wordcloud package\n",
    "    word_frequencies = { vocabulary_news_headlines[idx]:prob for idx, prob in enumerate(feature_distribution) }\n",
    "    show_wordcloud(word_frequencies, max_words=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Topic modeling, a key technique in natural language processing, is employed to uncover latent themes or topics within a collection of documents without requiring prior labeling. Among the prominent algorithms, Latent Dirichlet Allocation (LDA) stands out for its ability to identify these hidden topics. LDA operates under the assumption that each document comprises a blend of various topics, and each word within a document is linked to one of these topics.\n",
    "\n",
    "The process of topic modeling involves several stages. Initially, text data undergoes preprocessing, including tasks such as cleaning, tokenization, and removing irrelevant elements like stop words. Subsequently, documents are transformed into numerical vectors using methods like the bag-of-words model or TF-IDF. LDA is then applied to these vectors to detect underlying topics by examining word co-occurrences across documents. These identified topics are represented as distributions of words.\n",
    "\n",
    "The utility of topic modeling extends across diverse domains such as information retrieval, content recommendation systems, sentiment analysis, and the extraction of trends from extensive text datasets. By organizing and summarizing textual information, topic modeling aids in understanding the underlying themes present within large volumes of text, facilitating easier navigation and interpretation of complex textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

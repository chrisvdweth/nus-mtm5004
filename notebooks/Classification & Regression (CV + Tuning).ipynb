{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/div/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification & Regression: Cross Validation & Hyperparameter Tuning\n",
    "\n",
    "Cross-validation is a widely used technique in machine learning for evaluating the performance of a predictive model and estimating its generalization ability. It helps assess how well a model will perform on unseen data. Most classifiers or regressors feature a set of hyperparameters (e.g., the k in KNN) that can significantly affect the results. To find the best parameter settings, we have to train and evaluate for different parameter values.\n",
    "\n",
    "However, this evaluation of finding the best parameter values cannot be done using the test set. The test set has to be unseen using the very end for the final evaluation (once the hyperparameters have been fixed). Using the test set to tune the hyperparameters means that the test set has affected the training process.\n",
    "\n",
    "The process of cross-validation involves splitting the available dataset into multiple subsets or \"folds.\" One of the folds is used as the validation set, while the remaining folds are used for training the model. This process is repeated multiple times, with each fold serving as the validation set in a different iteration.\n",
    "\n",
    "Here's a step-by-step explanation of a common cross-validation procedure called \"k-fold cross-validation\":\n",
    "\n",
    "* The dataset is divided into k subsets or folds of approximately equal size.\n",
    "* The model is trained k times, each time using k-1 folds as the training data and one fold as the validation data.\n",
    "* The performance of the model is evaluated on each validation set, typically by calculating a performance metric such as accuracy, precision, recall, or F1 score.\n",
    "* The performance scores obtained from each fold are averaged to get an overall performance estimate of the model.\n",
    "\n",
    "The value of k can vary, but common choices include 5-fold or 10-fold cross-validation. Generally, a larger value of k leads to a more robust performance estimation but increases the computational cost. Once the model is validated using cross-validation and its performance is satisfactory, it can be trained on the entire dataset (without the need for validation sets) and used for making predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Specify How Plots Get Rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Cross-Validation (Single-Parameter Tuning)\n",
    "\n",
    "Let's first consider only a single hyperparameter for which we want to find the best value. This helps to focus on understanding the idea behind Cross-Validations. Throughout this notebook, we use the \"Vessels Details\" dataset and the task is to predict the `Type` of a vessel based on (some of) its features (e.g., `Length`, `Width`, etc.).\n",
    "\n",
    "### Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/datasets/vessels/vessel-details.csv')\n",
    "\n",
    "# Shuffling is ofte an good idea; the data might be sorted in some way\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Show the first 5 columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Selection\n",
    "\n",
    "To skip any more sophisticated data preprocessing steps, we consider only the convenient features -- that is, we consider only a subset of numerical features for our model. This particularly means that we do not have to consider any encoding strategies for categorical features. To keep it even simpler, we also remove all rows containing any missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the numerical attributes to keep it simple here + Type as our class label\n",
    "df = df[['Length', 'Width', 'Gross Tonnage', 'Deadweight Tonnage', 'Type']]\n",
    "\n",
    "# Remove all rows with any NaN values; again, just to keep it simple\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Class Labels\n",
    "\n",
    "Most classification algorithms assume that the class labels of the range 0..C, where C is the number of classes. Using `pandas`, this conversion is easy to do. After the conversion, all rows with the class labels, say, \"Oil Tanker\" will have the same numerical (integer) class label of the range 0..C. For our dataset, the number of classes is `C=15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type'] = pd.factorize(df['Type'])[0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Training & Test Data\n",
    "\n",
    "As usual, we convert the dataframe into numpy arrays for further processing, including splitting the dataset into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to numpy arrays\n",
    "X = df[['Length', 'Width', 'Gross Tonnage', 'Deadweight Tonnage']].to_numpy()\n",
    "y = df[['Type']].to_numpy().squeeze()\n",
    "\n",
    "# Split dataset in to training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))\n",
    "print(\"Size of test: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data via Standardization\n",
    "\n",
    "Since we want to consider different polynomial degrees, it is strongly recommended – and almost required – to normalize/standardize the data. As the [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) implementation also applies regularization by default, we do normalize the data via standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit the scaler based on the training data only\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Of course, we need to convert both training and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Logistic Classifier Using Cross-Validation\n",
    "\n",
    "#### Semi-Manually K-Fold Validation\n",
    "\n",
    "We first utilize `scikit-learn`'s  [`KFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) to split the training data into $k$ folds (here, $k=10$). The [`KFold.split()`] method generates the folds and allows to loop over all combinations of training and validation folds. Each combination contains $k-1$ training folds and 1 validation fold. For each combination we can retrain and validate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best f1-score and respective k value\n",
    "p_best, f1_best = None, 0.0\n",
    "\n",
    "# Loop over a range of values for setting p\n",
    "for p in tqdm(range(1, 10)):\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Transform data w.r.t to degree of polynomial p\n",
    "    poly = PolynomialFeatures(p)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train_poly):\n",
    "        \n",
    "        # Create the next combination of training and validation folds\n",
    "        X_trn, X_val = X_train_poly[train_index], X_train_poly[val_index]\n",
    "        y_trn, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "        # Train the classifier for the current training folds\n",
    "        classifier = LogisticRegression(fit_intercept=False, max_iter=1000).fit(X_trn, y_trn)\n",
    "        \n",
    "        # Predict the labels for the validation fold\n",
    "        y_pred = classifier.predict(X_val)\n",
    "\n",
    "        # Calculate the f1-score for the validation fold\n",
    "        f1_scores.append(f1_score(y_val, y_pred, average='micro'))\n",
    "        \n",
    "    # Calculate f1-score for all fold combination as the mean over all scores\n",
    "    f1_fold_mean = np.mean(f1_scores)\n",
    "    \n",
    "    # Keep track of the best f1-score and the respective k value\n",
    "    if f1_fold_mean > f1_best:\n",
    "        p_best, f1_best = p, f1_fold_mean\n",
    "        \n",
    "        \n",
    "print('The best average f1-score was {:.3f} for p={}'.format(f1_best, p_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Cross-Validation\n",
    "\n",
    "`scikit-learn` provides the even more convenient method [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) that does the generation of folds and splitting them into training folds and validation folds, as well as the training of a classifier for all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best f1-score and respective p value\n",
    "p_best, f1_best = None, 0.0\n",
    "\n",
    "\n",
    "# Loop over a range of values for setting p\n",
    "for p in tqdm(range(1, 9)):\n",
    "    \n",
    "    # Transform data w.r.t to degree of polynomial p\n",
    "    poly = PolynomialFeatures(p)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.fit_transform(X_test)    \n",
    "    \n",
    "    # Specfify type of classifier\n",
    "    classifier = LogisticRegression(fit_intercept=False, max_iter=1000)\n",
    "    \n",
    "    # perform cross validation (here with 5 folds)\n",
    "    # f1_scores is an array containg the 5 f1-scores\n",
    "    f1_scores = cross_val_score(classifier, X_train_poly, y_train, cv=5, scoring='f1_micro')\n",
    "    \n",
    "    # Calculate the f1-score for the current k value as the mean over all 10 f1-scores\n",
    "    f1_fold_mean = np.mean(f1_scores)\n",
    "    \n",
    "    # Keep track of the best f1-score and the respective k value\n",
    "    if f1_fold_mean > f1_best:\n",
    "        p_best, f1_best = p, f1_fold_mean\n",
    "  \n",
    "\n",
    "print('The best average f1-score was {:.3f} for a p={}'.format(f1_best, p_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side note:** The 2 previous code cells might yield different results for the best f1-score and the corresponding best value for $p$. This is because the splitting into folds has a random component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation on Test Data\n",
    "\n",
    "Now that we have identified the best value for $k$, we can perform the final evaluation using the test data. We can now also use the fill training data, and don't need to split it into any folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data w.r.t to degree of polynomial p\n",
    "poly = PolynomialFeatures(p_best)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)  \n",
    "\n",
    "classifier = LogisticRegression(fit_intercept=False, max_iter=1000).fit(X_train_poly, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_poly)\n",
    "\n",
    "f1_final = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print('The final f1-score of the Logistic Regression classifier (p={}) is: {:.3f}'.format(p_best, f1_final))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final score is the one to report when quantifying the quality of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Cross-Validation (Multi-Parameter Tuning)\n",
    "\n",
    "So far, we use Cross-Validation to find the best value for a single hyperparameter (here: the degree `p` of the polynomial). In practice, however, there might be many possible parameters we can and need to consider. This most commonly includes any hyperparameters of a classification model. For example, if you check the docs of [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) you can see hyperparameters such as\n",
    "\n",
    "* `penalty: {'l1', 'l2', 'elasticnet', None}, default='l2'`\n",
    "* `C: float, default=1.0` (inverse of regularization strength; must be a positive float)\n",
    "* `max_iter: int, default=100` (maximum number of iterations taken for the solvers to converge)\n",
    "\n",
    "and others.\n",
    "\n",
    "In principle, we could check all possible parameter combinations we want to consider the same way we did above by using nested loops to generate all combinations. For example, the example below shows how we can generate various parameter combinations using nested loops, with one loop for each of the hyperparameters we have just mentioned above.\n",
    "\n",
    "```\n",
    "for p in range(1, 9):\n",
    "\tfor penalty in ['l1', 'l2', 'elasticnet']:    \n",
    "    \tfor C in [0.1, 1, 10]:   \t \n",
    "        \tfor max_iter in [100, 1000, 2000]:       \t \n",
    "            \t...\n",
    "            \t# Fit and evaluate model with current parameter set\n",
    "            \t...\n",
    "```\n",
    "\n",
    "While this would work fine, it can be quite tedious and error prone to write such code. In practice, it is therefore much more convenient to use off-the-shelf auxiliary methods to simplify hyperparameter tuning. In the following, we go through some basic examples to illustrate this.\n",
    "\n",
    "### Create Model Pipeline\n",
    "\n",
    "Creating a classifier typically involves many steps such as preprocessing (e.g., standardization, generating polynomial features) and selecting and training a model. By using a [`sklearn.pipeline.Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) we can wrap all involved steps into a single entity. The code cell below shows an example where we create a pipeline that includes\n",
    "\n",
    "* `StandardScaler()` for standardizing the data\n",
    "* `PolynomialFeatures()` for the generation of polynomial features\n",
    "* `LogisticRegression` for training/fitting a Logistic Regression model\n",
    "\n",
    "Note how we give each component a name (e.g., `stdscaler`). The reason for this will be clearer in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('stdscaler', StandardScaler()),\n",
    "                       ('polyfeatures', PolynomialFeatures()),\n",
    "                       ('logreg', LogisticRegression(fit_intercept=False))\n",
    "                      ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Cross-Validations\n",
    "\n",
    "Another very useful auxiliary class is [`sklearn.model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) which perform Cross-Validation \"under the hood\". `GridSearchCV` also inspects as input the information about possible values for specified hyperparameters; see the variable `param_grid` in the code cell above.\n",
    "\n",
    "This dictionary allows us to specify the hyperparameter values. Hyperparameters are identified by name, where name is a combination of the component name in the pipeline and the parameter name of the component. For example, `logreg__penalty` refers to the `penalty` input parameter for the `LogisticRegression()` class we named `logreg` in our pipeline.\n",
    "\n",
    "In the code cell below, we use this approach to mimic the tuning we did previously, Note that only `polyfeatures__degree` refers to a set of different values while the remaining parameters `logreg__C`, `logreg__penalty`, and `logreg__max_iter` only refer to the same respective values we used above. With this, we have everything to perform Cross-Validation for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Define considered values for each considered hyperparameter\n",
    "param_grid = {'polyfeatures__degree': range(1, 10),\n",
    "              'logreg__C': [1],\n",
    "              'logreg__penalty': ['l2'],\n",
    "              'logreg__max_iter': [1000]\n",
    "             }\n",
    "\n",
    "# Perfom 5-fold Cross-Validation for each possible parameter combination\n",
    "grid_search_cv = GridSearchCV(pipe, param_grid=param_grid, cv=5, verbose=3).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above fits 45 models, since we have $9\\cdot 1 \\cdot 1 \\cdot 1 = 9$ possible parameter combinations and use $5$-fold Cross-Validation; hence, $9 \\cdot 5 = 45$.\n",
    "\n",
    "Lastly, we can simply inspect which combination of values for all hyperparameter values showed the best performance. Since we only varied the degree of the polynomial, the result should match the one we got before, i.e., that `p=5` gives us the best results for our data and task here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, ideally we want to consider many more parameter combinations by also varying the values of other hyperparameters. The code cell below shows an example. Since consider $3\\cdot 3\\cdot 3\\cdot 3 = 81$ possible parameter combinations in this combination, and still perform $5$-fold Cross Validation, we now need to fit $81 \\cdot 5 = 405$ models. While this is all done \"under the hood\" -- and as such does not require writing more code -- it naturally does greatly affect the overall runtime.\n",
    "\n",
    "As such, the code cell below will take some time to complete. Note that here we set `verbose=1` to avoid printing a line for each model that is fitted. However, if you want to see those lines to better observe the progress, you can change it to `verbose=3` like in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = {'polyfeatures__degree': [4, 5, 6],\n",
    "              'logreg__C': [0.1, 1, 10],\n",
    "              'logreg__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'logreg__max_iter': [100, 1000, 2000]\n",
    "             }\n",
    "\n",
    "grid_search_cv = GridSearchCV(pipe, param_grid=param_grid, cv=5, verbose=1)\n",
    "\n",
    "grid_search_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, we can check out the best values for all hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Cross-validation plays a crucial role in machine learning as it serves two primary purposes: model evaluation and hyperparameter tuning. By evaluating the model's performance using cross-validation, we can estimate its ability to generalize to unseen data and compare it against other models or algorithms. This evaluation helps in selecting the best-performing model for deployment.\n",
    "\n",
    "One of the key challenges in machine learning is overfitting, where a model learns to perform well on the training data but fails to generalize to new data. Cross-validation helps mitigate this issue by providing a more reliable estimate of the model's performance. By repeatedly training and evaluating the model on different subsets of the data, cross-validation helps identify models that generalize well.\n",
    "\n",
    "Another challenge in machine learning is the selection of optimal hyperparameters. Hyperparameters are parameters that are not learned from the data but are set manually by the user, such as the learning rate in neural networks or the depth of a decision tree. Cross-validation can be used to tune these hyperparameters by evaluating the model's performance with different combinations of hyperparameter values. This helps in finding the best set of hyperparameters that optimize the model's performance.\n",
    "\n",
    "However, cross-validation also presents certain challenges. One such challenge is the potential for data leakage. Data leakage occurs when information from the validation set inadvertently influences the model during training, leading to overly optimistic performance estimates. Care must be taken to ensure that the validation set remains independent and untouched during the training process.\n",
    "\n",
    "Another challenge is computational cost. Cross-validation involves training and evaluating the model multiple times, which can be time-consuming and computationally expensive, especially for large datasets or complex models. This challenge can be mitigated by using techniques like stratified sampling or parallel computing to improve efficiency.\n",
    "\n",
    "In summary, cross-validation is a vital technique in machine learning for model evaluation and hyperparameter tuning. It helps estimate a model's generalization performance, identify overfitting, and select optimal hyperparameters. While challenges like data leakage and computational cost exist, proper implementation and careful consideration of these challenges can ensure the effectiveness and reliability of cross-validation in improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

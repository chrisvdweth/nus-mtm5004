{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9628de95-e667-495c-9646-11237df9c0b7",
   "metadata": {},
   "source": [
    "<img src=\"data/images/div/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19d0a9-13f7-408c-82f8-ae7dbef28125",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "The Naive Bayes (NB) classifier -- here more specifically: Multinomial Naive Bayes (MNB) classifier -- is a probabilistic machine learning model based on Bayes' theorem. It's particularly well-suited for text classification tasks, especially when dealing with word frequencies or occurrence counts within text documents. Here's an overview of the Multinomial Naive Bayes classifier and its application in text classification:\n",
    "\n",
    "* **Bayes' Theorem:** The classifier is based on Bayes' theorem, which calculates the probability of a label (class) given observed features (words in the document). In text classification, it computes the probability of a document belonging to a particular category or sentiment class based on the occurrence of words in the document.\n",
    "\n",
    "* **Multinomial Distribution:** MNB assumes that the features (word counts or frequencies) follow a multinomial distribution. It works well with discrete features, such as word counts in text documents.\n",
    "\n",
    "* **Naive Bayes Assumption:** The \"naive\" assumption in MNB refers to the independence assumption between features (words in this context). It assumes that the presence or absence of each word in the document is independent of the presence or absence of other words, given the class label. While this assumption might not hold true in reality, MNB often performs well in practice, especially for text classification tasks.\n",
    "\n",
    "* **Text Classification:** In text classification, MNB uses the frequency of words (bag-of-words model) or other features derived from text (like TF-IDF - Term Frequency-Inverse Document Frequency) to build a probabilistic model. It calculates the likelihood of each word occurring in a particular class based on the training data. When a new document is encountered, it uses Bayes' theorem to calculate the probability of the document belonging to each class and selects the class with the highest probability as the predicted label.\n",
    "\n",
    "* **Sparse Data Handling:** MNB is robust in handling high-dimensional and sparse datasets typical in text classification. It works well even with relatively small training datasets and can efficiently handle a large number of features (words) without overfitting.\n",
    "\n",
    "Overall, the Multinomial Naive Bayes classifier is a simple yet effective probabilistic model for text classification tasks. Its ease of implementation, efficiency with sparse data, and reasonable performance, especially in tasks like document classification, spam filtering, and sentiment analysis, make it a popular choice in the field of natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767575d-c9d2-495f-88c8-5e9a0c5947d4",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bab8a-49d2-46a4-b17f-79dcb5da5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ee3e4-6563-4cb6-98ae-8a507fe3de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95605cb-d78c-4176-aff3-89f787d4c559",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "For this notebook, we use a simple dataset for sentiment classification. This dataset consists of 10,662 sentences, where 50% of the sentences are labeled 1 (positive), and 50% of the sentences are labeled -1 (negative).\n",
    "\n",
    "### Loading Sentence/Label Pairs from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4014a-4e71-4705-b448-35ed3ffb22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = [], []\n",
    "\n",
    "with open(\"data/datasets/sentence-polarities/sentence-polarities.csv\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        sentence, label = line.split(\"\\t\")\n",
    "        sentences.append(sentence)\n",
    "        labels.append(int(label))  \n",
    "        \n",
    "print(\"Total number of sentences: {}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728c8fc-19f5-46b8-b4ae-c5020a54c0d4",
   "metadata": {},
   "source": [
    "### Create Training & Test Set\n",
    "\n",
    "To evaluate any classifier, we need to split our dataset into a training and a test set. With the method `train_test_split()` this is very easy to do; this method also shuffles the dataset by default, which is important for this example, since the dataset file is ordered with all positive sentences coming first. In the example below, we set the size of the test set to 20%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8e763-c7f7-44db-ab32-e799eab2ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentences and labels into training and test set with a test set size of 20%\n",
    "sentences_train, sentences_test, labels_train, labels_test = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# We can directly convert the numerical class labels from lists to numpy arrays\n",
    "y_train = np.asarray(labels_train)\n",
    "y_test = np.asarray(labels_test)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(sentences_train)))\n",
    "print(\"Size of test set: {}\".format(len(sentences_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec73c2-0567-46da-9f04-9393e8a933ce",
   "metadata": {},
   "source": [
    "## Training & Testing a Naive Bayes Classifier\n",
    "\n",
    "Let's first have a look at how to train a Naive Bayes classifier with the minimum number of steps. For this, we randomly pick some meaningful values for the vectorizer and use the the default values of the [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5cb848-5324-4f6c-bb57-8fa12f965b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document-Term Matrix for differen n-gram sizes\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(sentences_train)\n",
    "X_test = tfidf_vectorizer.transform(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef9346-5773-4d17-9bbc-474cd60df1f4",
   "metadata": {},
   "source": [
    "Using the training data, we can train a Naive Bayes classifier with a single line of code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd90a07-2ef2-4085-a070-4a62f2c57636",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db8bf4-dbdb-41cd-9851-8c073a845e1f",
   "metadata": {},
   "source": [
    "Once trained, we can predict the class labels for the document vectors in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c60195-54ee-4982-82dc-ffe902a9b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36f6439-b444-4477-921b-47e8aaf6acef",
   "metadata": {},
   "source": [
    "`y_pred` now contains the 2,133 predicted labels that we can compare with the ground truth labels from the test set. scikit-learn provides methods to easily calculate all the important metrics we covered in the lecture. Since we only have to class labels (i.e., binary classification), we do not have to set the `average` parameter to indicate micro or macro averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03669c7f-8804-439e-bdc2-bbb92029db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precison: {:.3f}\".format(precision))\n",
    "print(\"Recall:   {:.3f}\".format(recall))\n",
    "print(\"F1 score: {:.3f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13cf18-70b3-4df1-8172-c27c79c740a8",
   "metadata": {},
   "source": [
    "scikit-learn also provides a method `classification_report()` for a more detailed description of the results, showing a breakdown of the precision, recall, and f1 scores broken down for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2eae60-292b-43ed-879f-f10e6f063126",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb070f85-59c5-4ef6-b6aa-e771a0ca6445",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "The Naive Bayes Classifier -- compared to, e.g., the K-Nearest Neighbor Classifier -- has no fundamentally intrinsic parameter that needs to be chosen wisely. If you check the documentation of [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) you will see some input parameters. However, there are not as fundamental as, say, the `n_neigbors` for [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393bb0a-759c-4616-9bcf-07695d21f798",
   "metadata": {},
   "source": [
    "### Selecting the Best Maximum N-Gram Size\n",
    "\n",
    "In the case of the Naive Bayes Classifier, the input in terms of the size of the n-gram has typically the most effect on the results. In the basic example above, we only assumed unigram. Now let's see how the result changes if we change the maximum number of n-gram when vectorizing our input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037939d-a926-43de-8e01-9979dce54d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ngram_size = 1\n",
    "max_ngram_size = 5\n",
    "\n",
    "num_runs = max_ngram_size - min_ngram_size\n",
    "\n",
    "# numpy array to keep track of all results\n",
    "results = []\n",
    "\n",
    "with tqdm(total=num_runs) as pbar:\n",
    "    for i, ngram in enumerate(range(min_ngram_size, max_ngram_size+1)):\n",
    "        # Create Document-Term Matrix for different n-gram sizes\n",
    "        tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, ngram), max_features=20000)\n",
    "        X_train = tfidf_vectorizer.fit_transform(sentences_train)\n",
    "        X_test = tfidf_vectorizer.transform(sentences_test)\n",
    "        # Train & test model using cross validation\n",
    "        model = MultinomialNB()\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=10, scoring=\"f1\")\n",
    "        mean_score = np.mean(scores)\n",
    "        results.append((ngram, mean_score))\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3272a6-2ba1-48c6-b9dd-8e67439194a5",
   "metadata": {},
   "source": [
    "With the f1 scores for the different values for `max_ngram_size`, we can quickly plot those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0074f52-575e-4b32-b34f-d8f894692be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([s[0] for s in results], [s[1] for s in results], lw=3)\n",
    "font_axes = {'family':'serif','color':'black','size':16}\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"Max N-Gram Size\", fontdict=font_axes)\n",
    "plt.ylabel(\"F1 Score\", fontdict=font_axes)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ace1a-ac44-4769-a922-0ccbdecfd2e4",
   "metadata": {},
   "source": [
    "While the best value for the maximum n-gram size is at 2, keep in mind that the f1 score actually doesn't change too much; see the scale of the y-axis. The main reason for this is that, for example, a maximum n-gram size of 3 still contains all unigrams and bigrams. This is the most common approach in practice. However, feel free to also set `min_ngram_size` to a larger value than 1 and see how it affects the results.\n",
    "\n",
    "Of course, all these results and observations only hold true for this specific data set and might significantly differ for other ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d086fbb-fb04-4ea0-92d2-263ad01cdbbe",
   "metadata": {},
   "source": [
    "## Pipelines & Grid Search\n",
    "\n",
    "Hyperparameter tuning is a quite important step, but the previous example has shown that it can be quite tedious. However, note that we basically tried all possible combinations for certain sets of parameter values. And since we were tuning 2 parameters, we required 2 nested loops. Thus, if we would tune $N$ parameters at the same time, we would need to have $N$ nested loops. Luckily, scikit-learn makes this much easier using [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "Since the parameters we would like to tune refer to 2 different components -- the vectorizer and the classifier -- we also need a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to combine both components into a single model. Let's do this first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c7b3e-1420-47c1-9754-2758b5d5684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a091c-297e-4b3c-aa6e-74cdb624eeae",
   "metadata": {},
   "source": [
    "Now we can define the search space, by providing the set of values for the hyperparameters we want to consider. See how the identifier of the parameters are a combination of the name in the pipeline (here: `tfidf` and `nb`) and the name of the parameter in the respective class. For example, `tfidf__max_df` refers to the `max_df` parameter of the `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e536653-6517-441d-be24-3c04b0324547",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.75, 1.0),\n",
    "    'tfidf__max_features': (5000, 10000),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'nb__alpha': (0.5, 1.0, 1.5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc595c-5ddb-4f59-8d8a-9814d9908792",
   "metadata": {},
   "source": [
    "Now we can use `GridSearchCV` to check all possible combinations of parameter values. Of course, we kept the number of possible values rather small to avoid overly run times here. Note that for any parameter not listed above (e.g., `min_df` of the vectorizer) the default value is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7fcb4-df11-49e9-970e-27a027235968",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=2, cv=5)\n",
    "\n",
    "grid_search = grid_search.fit(sentences_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698655b5-2a5c-4bea-9344-6e3cb4f2e97a",
   "metadata": {},
   "source": [
    "Once the `GridSearchCV` has checked all possible parameter combinations, we can read out the best combination as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7777bc-075f-4431-930b-031299a7f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e63200-c031-44e0-9778-b9fe3070a1cf",
   "metadata": {},
   "source": [
    "With these best parameter values -- note that those might not really be the best values as we selected just some alternatives for this example -- we compute the final scores by vectorizing our data and training the Naive Bayes Classifiers using those parameters. Now we train the classifier using the complete training data, and evaluate the classifier over the test data. Appreciate that we used the test data only this one time for the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6797bf5-5e9d-44cf-ad38-304de3ca2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=10000, max_df=0.75)\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(sentences_train)\n",
    "X_test = tfidf_vectorizer.transform(sentences_test)\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precison: {:.3f}\".format(precision))\n",
    "print(\"Recall:   {:.3f}\".format(recall))\n",
    "print(\"F1 score: {:.3f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86004b0-2961-47ac-a8f0-b40d54da90a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b665c9-7529-492d-babd-45d3b7c30ebe",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we looked at the Naive Bayes classifier. Of course, with packages like scikit-learn, it is very easy to train a classifier with very few lines of code. We saw that the Naive Bayes classifier is in some sense very easy to train as it does not feature any very fundamental parameters that need to be tuned. Despite its simplicity, this model can still provide good results for text classification and can serve as a simple baseline to compare against more sophisticated models for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094a0cd-deaf-474c-b863-218efa5ccaef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

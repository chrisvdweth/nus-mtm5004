{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eeb7db4-03bb-4a57-ba94-da5e8263162a",
   "metadata": {},
   "source": [
    "<img src=\"data/images/div/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f415b-0a2d-4fc5-81a6-5ffd853a032a",
   "metadata": {},
   "source": [
    "# Sentiment Analysis -- Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to effectively handle sequential data by retaining memory or context of previous inputs. Unlike feedforward neural networks that process data in fixed-size input vectors, RNNs have loops within their architecture, allowing them to maintain and utilize information about previous inputs while processing the current input.\n",
    "\n",
    "RNNs are composed of units (often called cells) that maintain a hidden state. This hidden state acts as a memory that retains information about previous inputs in the sequence. Each unit performs computations based on the current input and its previous hidden state, allowing them to capture temporal dependencies in sequential data.\n",
    "\n",
    "For text classification tasks, RNNs can be used in various ways:\n",
    "\n",
    "* **Word-level RNNs:** Each word in a text sequence is fed into the RNN step by step. The hidden state of the RNN unit at each step incorporates information about the previous words in the sequence. This way, the RNN learns to capture the context and dependencies between words in the text.\n",
    "\n",
    "* **Sequence-to-Sequence RNNs:** These models take an entire sequence as input and produce another sequence as output. In text classification, this could involve using an RNN to read an entire sentence or document and outputting a sentiment label or category.\n",
    "\n",
    "* **Sentiment Analysis:** In sentiment analysis, RNNs can be employed to classify the sentiment of text documents (positive, negative, neutral). The RNN processes the words or sequences of words in the document, learning patterns and relationships to determine the sentiment expressed.\n",
    "\n",
    "However, vanilla RNNs suffer from issues like vanishing or exploding gradients, which can hinder their ability to capture long-term dependencies in text. To address these limitations, variants of RNNs, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), have been developed. These architectures have gating mechanisms that control the flow of information, allowing them to better capture long-range dependencies in text sequences.\n",
    "\n",
    "In summary, RNNs, including LSTM and GRU variants, are powerful for text classification tasks because they can capture sequential dependencies, understand context, and make predictions based on the order and structure of text data. Their ability to retain memory and handle sequential information makes them well-suited for tasks where understanding the context of words or phrases is essential, such as sentiment analysis, named entity recognition, machine translation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821ac59-d8ef-4934-a49b-f3a08068dc08",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79138794-1ed9-4315-a122-77bca80859d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71fb95-a99c-42fb-a98e-ebb72a64c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "# Custom BatchSampler\n",
    "from src.sampler import EqualLengthsBatchSampler\n",
    "from src.utils import Dict2Class, plot_training_results\n",
    "from src.rnn import RnnType, RnnTextClassifier, DotAttentionClassification\n",
    "\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "# We use spaCy for preprocessing, but we only need the tokenizer and lemmatizer\n",
    "# (for a large real-world dataset that would help with the performance)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbcc76-19d3-464e-97ad-a972e4ea10e6",
   "metadata": {},
   "source": [
    "### Checking/Setting the Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPU to significantly speed up the training process. If you have a support GPU, feel free to utilize it. However, for this notebook it's certainly not needed as our dataset is small and our network model is very simple. In fact, the training is fast on the CPU here since initializing memory on the GPU and moving the data to the GPU involves some overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f4cd4-66f7-48b8-b989-87399386266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU (in case you don't have a supported GPU)\n",
    "# With this small dataset and simple model you won't see a difference anyway\n",
    "#use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de897744-74c1-44b4-a28c-28c93a8f2b2b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276dae19-d787-45ca-80d9-2110291f0ec4",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "### Sentence Polarity Datset\n",
    "\n",
    "The [sentence polarity dataset](https://www.kaggle.com/datasets/nltkdata/sentence-polarity) is a well-known dataset commonly used for sentiment analysis and text classification tasks in NLP. It consists of sentences or short texts labeled with their corresponding sentiment polarity (positive or negative). This dataset is often used to train and evaluate models that aim to classify text into positive or negative sentiment categories. It serves as a benchmark for sentiment analysis tasks and provides a standardized dataset for researchers and practitioners to compare and evaluate the performance of different algorithms and techniques.\n",
    "\n",
    "There are several versions and variations of the sentence polarity dataset available, created for different purposes and domains. One of the popular versions is the Movie Review Dataset, also known as the Pang and Lee dataset, created by Bo Pang and Lillian Lee. This dataset contains movie reviews from the website IMDb, with each review labeled as positive or negative. The sentence polarity dataset enables researchers and developers to build and test sentiment analysis models that can automatically determine the sentiment expressed in text, allowing applications such as sentiment monitoring, opinion mining, and customer feedback analysis.\n",
    "\n",
    "For this notebook, we already prepared the dataset by combining the 2 files containing the positive and negative sentences into a single file. The polarity of each sentence is denoted by a polarity label: `1` for positive and `-1` for negative. This makes handling the data a bit simpler and keeps the notebook a bot cleaner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e4211-f0d0-41bc-a6fa-4dd0c5249a94",
   "metadata": {},
   "source": [
    "### Auxiliary Method\n",
    "\n",
    "The method `preprocess()`, well, tokenizes a given text. In this case, we not only tokenize but also lemmatize and lowercase all tokens. The exact list of preprocessing steps will in practice depend on the exact task, but this is what we do here. Notice that we do not, for example, remove stopwords. This is mainly to reduce the vocabulary size not too much here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3270d-087b-450f-88da-8c55afdd5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return [token.lemma_.lower() for token in nlp(text)]\n",
    "\n",
    "preprocess(\"This is a test to see if the TOKENIZER does its job.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cee28-f5c0-484d-85ca-1fc00b6bb014",
   "metadata": {},
   "source": [
    "### Read Files & Compute Word Frequencies\n",
    "\n",
    "The first to go through the whole corpus and count the number of occurrences for each token. 10k sentences is basically nothing these days, but the purpose of this notebook is not to focus on large scale data as the steps would be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e66ce-d95e-4af9-a8f7-b5414ba0fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "\n",
    "targets_polarity = []\n",
    "\n",
    "with tqdm(total=10662) as pbar:\n",
    "    \n",
    "    # Loop over each sentence (1 sentence per line)\n",
    "    with open('data/datasets/sentence-polarities/sentence-polarities.csv', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            parts = line.split('\\t')\n",
    "            sentence, label = parts[0], int(parts[1])\n",
    "            # Update token counts\n",
    "            for token in preprocess(sentence):\n",
    "                token_counter[token] += 1            \n",
    "            # Add label to targets list\n",
    "            targets_polarity.append(label)\n",
    "            # Update progress bar\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba6aa6-c58b-4350-8167-5b69af86c472",
   "metadata": {},
   "source": [
    "### Create Vocabulary\n",
    "\n",
    "To create our `vocab` object, we perform exactly the same steps as above. The only difference is that our \"full\" vocabulary is not larger (although with less than 20k tokens still rather small). We therefore limit the vocabulary here to the 10,000 most frequent tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01054b34-7e6f-4d81-a25c-3129ec145f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by word frequency\n",
    "token_counter_sorted = sorted(token_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Number of tokens: {}\".format(len(token_counter_sorted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ca68c-9e00-4235-871a-2f85f4a4415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_TOKENS = 10000\n",
    "\n",
    "token_counter_sorted = token_counter_sorted[:TOP_TOKENS]\n",
    "\n",
    "print(\"Number of tokens: {}\".format(len(token_counter_sorted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696fe29-750e-41ff-96d4-50cfb26bf43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ordered_dict = OrderedDict(token_counter_sorted)\n",
    "\n",
    "# Define list of \"special\" tokens\n",
    "SPECIALS = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
    "\n",
    "vocabulary = vocab(token_ordered_dict, specials=SPECIALS)\n",
    "\n",
    "vocabulary.set_default_index(vocabulary[\"<UNK>\"])\n",
    "\n",
    "print(\"Number of tokens: {}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9b777-c5e1-4f7c-86b1-f81b3b376364",
   "metadata": {},
   "source": [
    "### Save Dataset\n",
    "\n",
    "Lastly, we save all the data for later use, and so we don't have to recompute it every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2ae1f-4e02-4d12-8c79-6b408e65b2f4",
   "metadata": {},
   "source": [
    "#### Vectorize and Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5886a481-11a7-4bcc-8938-55bca68cf590",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"data/datasets/sentence-polarities/polarity-dataset-vectors-{}.txt\".format(TOP_TOKENS), \"w\")\n",
    "\n",
    "with tqdm(total=10662) as pbar:\n",
    "    \n",
    "    # Loop over each sentence (1 sentence per line)\n",
    "    with open('data/datasets/sentence-polarities/sentence-polarities.csv', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            parts = line.split('\\t')\n",
    "            sentence, label = parts[0], int(parts[1])\n",
    "            # Convert labels from -1/1 to 0/1\n",
    "            label = int((label + 1) / 2)\n",
    "            # Convert sentence into sequence of word indices\n",
    "            vector = vocabulary.lookup_indices(preprocess(sentence))\n",
    "            # Write converted sequence and labelsto file\n",
    "            output_file.write(\"{}\\t{}\\n\".format(\" \".join([str(idx) for idx in vector]), label))\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "output_file.flush()\n",
    "output_file.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662707b4-4d7c-42b9-80f9-318e5cb5125f",
   "metadata": {},
   "source": [
    "#### Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb2402-a274-42ff-b39f-0d3688674c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file_name = \"data/datasets/sentence-polarities/polarity-corpus-{}.vocab\".format(TOP_TOKENS)\n",
    "\n",
    "torch.save(vocabulary, vocabulary_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5534e31-c71b-4478-bdbb-524c371076bc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1021e-dbb7-4d8e-94db-5c37d0e0ea72",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Training\n",
    "\n",
    "While RNNs allow for arbitrary lengths -- as long as sequence in the same batch is of the same length -- it is often practical to limited the maximum length of sequences. This is not only from a computing point of view but also it gets more and more difficult to propagate meaningful gradients back during Backpropgation Throught Time (BPTT).\n",
    "\n",
    "For the sentence dataset, this is hardly an isses, since individual sentences are usually not overly long. However, the moview reviews consiste of several sentences. Note that by limiting ourselves to the first `MAX_LENGTH` words we assume that the main sentiment is expressed at the beginning of the review. If we would assume that we should focus on the end of a review, we should consider the last `MAX_LENGTH` words. \n",
    "\n",
    "In the code cell below, we set `MAX_LENGTH` to 10, but feel free to play with this value. When loading the data from the files, we directly cut all sequences longer than `MAX_LENGTH` down to the specified values. This also means that we won't have to check the seqquence lengths anymore when training or evaluating a model (compared to CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38facef-5211-4345-8c02-3fd7aecc351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66b14c-3c6f-4144-8d27-c691b612af86",
   "metadata": {},
   "source": [
    "### Dataset A: Sentence Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f088595-f1c3-4ec5-8dd4-cc1148772578",
   "metadata": {},
   "source": [
    "#### Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b5a2e-7a13-4613-a369-290eca1c599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = torch.load(\"data/datasets/sentence-polarities/polarity-corpus-10000.vocab\")\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print(\"Size of vocabulary:\\t{}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1614053-7b39-484c-90af-8775d6d870d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, targets = [], []\n",
    "\n",
    "with open(\"data/datasets/sentence-polarities/polarity-dataset-vectors-10000.txt\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        # The input sequences and class labels are separated by a tab\n",
    "        sequence, label = line.split(\"\\t\")\n",
    "        # Convert sequence string to a list of integers (reflecting the indicies in the vocabulary)\n",
    "        sequence = [ int(idx) for idx in sequence.split()]\n",
    "        # Convert each sequence into a tensor\n",
    "        sequence = torch.LongTensor(sequence[:MAX_LENGTH])\n",
    "        # Add sequence and label to the respective lists\n",
    "        sequences.append(sequence)\n",
    "        targets.append(int(label))\n",
    "        \n",
    "# As targets is just a list of class labels, we can directly convert it into a tensor\n",
    "targets = torch.LongTensor(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6e879-6234-4b45-914e-3a10e54b8c58",
   "metadata": {},
   "source": [
    "### Create Training & Test Set\n",
    "\n",
    "To evaluate any classifier, we need to split our dataset into a training and a test set. With the method `train_test_split()` this is very easy to do; this method also shuffles the dataset by default, which is important for this example, since the dataset file is ordered with all positive sentences coming first. In the example below, we set the size of the test set to 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef731b-ee11-44c4-804c-2f223fc3f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=0.5, shuffle=True, random_state=0)\n",
    "\n",
    "print(\"Number of training samples:\\t{}\".format(len(X_train)))\n",
    "print(\"Number of test samples:\\t\\t{}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccf913-3d54-43bc-a681-933436a7e7a6",
   "metadata": {},
   "source": [
    "### Create Dataset Class\n",
    "\n",
    "We first create a simple [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). This class only stores out `inputs` and `targets` and needs to implement the `__len__()` and `__getitem__()` methods. Since our class extends the abstract class [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset), we can use an instance later to create a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Without going into too much detail, this approach does not only allow for cleaner code but also supports parallel processing on many CPUs, or on the GPU as well as to optimize data transfer between the CPU and GPU, which is critical when processing very large amounts of data. It is therefore the recommended best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c274ed-191f-4acf-aacc-7fa9dcea9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.targets is None:\n",
    "            return np.asarray(self.inputs[index])\n",
    "        else:\n",
    "            return np.asarray(self.inputs[index]), np.asarray(self.targets[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164f976-282d-4a14-881c-2df422be887e",
   "metadata": {},
   "source": [
    "### Create Data Loaders\n",
    "\n",
    "The [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class takes a `DataSet` object as input to handle to split the dataset into batches. The class `EqualLengthsBatchSampler` analyzes the input sequences to organize all sequences into groups of sequences of the same length. Then, each batch is sampled for a single group, ensuring that all sequences in the batch have the same length. In the following, we use a batch size of 256, although you can easily go higher since we are dealing with only sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dffa6e-2aa4-4516-b037-70a583f4b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "dataset_train = BaseDataset(X_train, y_train)\n",
    "sampler_train = EqualLengthsBatchSampler(batch_size, X_train, y_train)\n",
    "loader_train = DataLoader(dataset_train, batch_sampler=sampler_train, shuffle=False, drop_last=False)\n",
    "\n",
    "dataset_test = BaseDataset(X_test, y_test)\n",
    "sampler_test = EqualLengthsBatchSampler(batch_size, X_test, y_test)\n",
    "loader_test = DataLoader(dataset_test, batch_sampler=sampler_test, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada40ba5-55b7-43be-aca2-9364a20d99b5",
   "metadata": {},
   "source": [
    "## Train & Evaluate Model\n",
    "\n",
    "### Auxiliary Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36db0b-5763-4513-b423-4b3ab9ff2bd3",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "The code cell below implements the method `evaluate()` to, well, evaluate our model. Apart from the model itself, the method also receives the data loader as input parameter. This allows us later to use both `loader_train` and `loader_test` to evaluate the training and test loss using the same method.\n",
    "\n",
    "The method is very generic and is not specific to the dataset. It simply loops over all batches of the data loader, computes the log probabilities, uses these log probabilities to derive the predicted class labels, and compares the predictions with the ground truth to return the f1 score. This means, this method could be used \"as is\" or easily be adopted for all kinds of classifications tasks (incl. task with more than 2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d8973-b913-419b-8ed6-e73b04b2f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with tqdm(total=len(loader)) as pbar:\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            batch_size, seq_len = X_batch.shape[0], X_batch.shape[1]\n",
    "            \n",
    "            # Move the batch to the correct device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Initialize the first hidden state h0 (and move to device)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "\n",
    "            if type(hidden) is tuple:\n",
    "                hidden = (hidden[0].to(device), hidden[1].to(device))  # LSTM\n",
    "            else:\n",
    "                hidden = hidden.to(device)  # RNN, GRU\n",
    "                    \n",
    "            log_probs = model(X_batch, hidden)\n",
    "\n",
    "            y_batch_pred = torch.argmax(log_probs, dim=1)\n",
    "\n",
    "            y_true += list(y_batch.cpu())\n",
    "            y_pred += list(y_batch_pred.cpu())\n",
    "            \n",
    "            pbar.update(batch_size)\n",
    "\n",
    "    return f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f7af5-8254-4a4c-a970-c0325b33803c",
   "metadata": {},
   "source": [
    "### Train Model (single epoch)\n",
    "\n",
    "Similar to the method `evaluate()` we also implement a method `train_epoch()` to wrap all the required steps training. This has the advantage that we can simply call `train_epochs()` multiple times to proceed with the training. Apart from the model, this method has the following input parameters:\n",
    "\n",
    "* `optimizer`: the optimizer specifier how the computed gradients are used to updates the weights; in the lecture, we only covered the basic Stochastic Gradient Descent, but there are much more efficient alternatives available\n",
    "\n",
    "* `criterion`: this is the loss function; \"criterion\" is just very common terminology in the PyTorch documentation and tutorials\n",
    "\n",
    "The hear of the method is the snippet described as PyTorch Magic. It consists of the following 3 lines of code\n",
    "\n",
    "* `optimizer.zero_grad()`: After each training step for a batch if have to set the gradients back to zero for the next batch\n",
    "\n",
    "* `loss.backward()`: Calculating all gradients using backpropagation\n",
    "\n",
    "* `optimizer.step()`: Update all weights using the gradients and the method of the specific optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396f9ae-5af6-467a-9223-ffa7b44563f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    \n",
    "    # Initialize epoch loss (cummulative loss fo all batchs)\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    with tqdm(total=len(loader)) as pbar:\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            batch_size, seq_len = X_batch.shape[0], X_batch.shape[1]\n",
    "\n",
    "            # Move the batch to the correct device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Initialize the first hidden state h0 (and move to device)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "\n",
    "            if type(hidden) is tuple:\n",
    "                hidden = (hidden[0].to(device), hidden[1].to(device))  # LSTM\n",
    "            else:\n",
    "                hidden = hidden.to(device)  # RNN, GRU            \n",
    "            \n",
    "            log_probs = model(X_batch, hidden)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(log_probs, y_batch)\n",
    "            \n",
    "            ### Pytorch magic! ###\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Keep track of overall epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            pbar.update(batch_size)\n",
    "            \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1b176-2acb-4753-bf18-720d3caff0ae",
   "metadata": {},
   "source": [
    "#### Train Model (multiple epochs)\n",
    "\n",
    "The `train()` method combines the training and evaluation of a model epoch by epoch. The method keeps track of the loss, the training score, and the tests score for each epoch. This allows as later to plot the results; see below. Notice the calls of `model.train()` and `model.eval()` to set the models into the correcte \"mode\". This is needed sinze our model containsa Dropout layer. For more details, check out this [Stackoverflow post](https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86bae9-9ef8-4cf8-b3ad-18566f5348ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader_train, loader_test, optimizer, criterion, num_epochs, verbose=False):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Total Training Time (total number of epochs: {})\".format(num_epochs))\n",
    "    #for epoch in tqdm(range(1, num_epochs+1)):\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = train_epoch(model, loader_train, optimizer, criterion)\n",
    "        model.eval()\n",
    "        f1_train = evaluate(model, loader_train)\n",
    "        f1_test = evaluate(model, loader_test)\n",
    "\n",
    "        results.append((epoch_loss, f1_train, f1_test))\n",
    "        \n",
    "        if verbose is True:\n",
    "            print(\"[Epoch {}] loss:\\t{:.3f}, f1 train: {:.3f}, f1 test: {:.3f} \".format(epoch, epoch_loss, f1_train, f1_test))\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34458c13-748e-4e7a-85e8-af656c17d372",
   "metadata": {},
   "source": [
    "### Basic RNN Model\n",
    "\n",
    "The class `RnnTextClassifier` implements an RNN-based classifier in a flexible manner, using different parameters setting once cna set:\n",
    "\n",
    "* Which recurrent cell to use: nn.RNN, nn.GRU, or nn.LSTM\n",
    "\n",
    "* The number of stacked recurrent layers\n",
    "\n",
    "* Whether the recurrence is performed bi-directional or not\n",
    "\n",
    "* The number and size of the subsequence linear layers\n",
    "\n",
    "* ... and other various parameters,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f250c-a730-4d1c-9b0b-8b363148e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embed_size\": 300,\n",
    "    \"rnn_type\": RnnType.GRU,\n",
    "    \"rnn_num_layers\": 2,\n",
    "    \"rnn_bidirectional\": True,\n",
    "    \"rnn_hidden_size\": 512,\n",
    "    \"rnn_dropout\": 0.5,      # only relevant if rnn_num_layers > 1\n",
    "    \"dot_attention\": False,\n",
    "    \"linear_hidden_sizes\": [128, 64],\n",
    "    \"linear_dropout\": 0.5,\n",
    "    \"output_size\": 2\n",
    "}\n",
    "\n",
    "# Define model paramaters\n",
    "params = Dict2Class(params)\n",
    "# Create model   \n",
    "rnn = RnnTextClassifier(params).to(device)\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0001)\n",
    "# Define loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba5cff-2a07-43f3-9ab2-aa8716e33066",
   "metadata": {},
   "source": [
    "#### Evaluate Untrained Model\n",
    "\n",
    "Let's first see how our model performs when untrain, i.e., with the initial random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b88e5-b624-4d81-880c-d0516db3f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rnn, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa6997-5956-4f70-9b19-efa9cc03842a",
   "metadata": {},
   "source": [
    "### Full Training (and evaluation after each epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740d46c-e28a-4952-ba03-ca57af7070ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "#train(basic_rnn_classifier, loader, num_epochs, verbose=True)\n",
    "results = train(rnn, loader_train, loader_test, optimizer, criterion, num_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67766399-fd0e-456d-a9f4-d8e72266cd87",
   "metadata": {},
   "source": [
    "In `src.utils` you can find the method `plot_training_results()` to plot the losses and accuracies (training + test) over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f803c7d-c4b1-4567-b940-471d1e26ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0e42c-dbb3-4b21-9ac7-8fca96eb59b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b347f6-f78c-4a71-be91-bc8912a634c1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Recurrent Neural Networks (RNNs) have emerged as a formidable tool for text classification tasks like sentiment analysis due to their intrinsic ability to understand sequential data and capture dependencies among words or characters within text sequences. RNNs, unlike traditional feedforward networks, maintain a memory state that allows them to retain information from previous inputs while processing the current input. This unique architecture enables them to capture temporal dependencies and contextual information crucial for understanding the sentiment or meaning conveyed in text.\n",
    "\n",
    "In sentiment analysis, RNNs excel at grasping the sequential nature of language, discerning nuances in meaning, and identifying sentiment-bearing words or phrases within sentences or documents. By processing text sequentially, RNNs effectively consider the order of words and their relationships, thus grasping the context necessary for accurate classification.\n",
    "\n",
    "However, traditional RNNs are prone to issues like vanishing or exploding gradients, limiting their ability to capture long-range dependencies effectively. To mitigate these shortcomings, variants like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) have been developed. These variants incorporate gating mechanisms that regulate the flow of information, allowing them to better retain relevant context over longer sequences.\n",
    "\n",
    "Overall, RNNs, including their specialized LSTM and GRU variants, stand out in text classification tasks like sentiment analysis by leveraging sequential information to comprehend context, relationships, and dependencies among words or characters within text data. Their strength lies in their capacity to handle sequences, making them a potent choice for tasks where understanding the sequential nature of language is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f832e-2035-413a-8957-1be3c568c78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
